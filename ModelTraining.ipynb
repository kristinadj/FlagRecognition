{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOaa6wbiJTzo1rpR8eE99Ug",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristinadj/FlagRecognition/blob/master/ModelTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svN5yvtu2RZB",
        "colab_type": "text"
      },
      "source": [
        "# **Training YOLOv3 object detection on a custom flag dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGG3Qwp127wD",
        "colab_type": "text"
      },
      "source": [
        "# Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6599CG683CPK",
        "colab_type": "code",
        "outputId": "8bb4b1e6-8733-4074-c02b-c712bca7ac05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip install keras==2.2.4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.17.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieNyeAUS3VUB",
        "colab_type": "code",
        "outputId": "7696a513-6f5e-4a78-b4f4-b9ef2002f62f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!python -c 'import keras; print(keras.__version__)'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wip5dla53dHe",
        "colab_type": "code",
        "outputId": "04594421-e3bf-4690-b3b0-bd76a0c15268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone https://github.com/josephofiowa/keras-yolo3.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-yolo3'...\n",
            "remote: Enumerating objects: 165, done.\u001b[K\n",
            "remote: Total 165 (delta 0), reused 0 (delta 0), pack-reused 165\u001b[K\n",
            "Receiving objects: 100% (165/165), 156.01 KiB | 868.00 KiB/s, done.\n",
            "Resolving deltas: 100% (79/79), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73LzB_EI3uaZ",
        "colab_type": "code",
        "outputId": "d343651d-c5e9-47ac-804d-1982c1dcb7a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd keras-yolo3/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/keras-yolo3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBTSXfhM377y",
        "colab_type": "text"
      },
      "source": [
        "# Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmHAZAlE4UBo",
        "colab_type": "code",
        "outputId": "b7bb46cc-3969-49e4-c86c-4a3cc7e6727e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!curl -L https://app.roboflow.ai/ds/jBWn24CLNz?key=LFlRAXjyOU | jar -x"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   892  100   892    0     0    997      0 --:--:-- --:--:-- --:--:--   997\n",
            "100 10.3M  100 10.3M    0     0  6632k      0  0:00:01  0:00:01 --:--:-- 20.7M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mHvw29Y6f8t",
        "colab_type": "code",
        "outputId": "5cdcd36c-cfb0-46fb-e4b4-8dfefb81361c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd export"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/keras-yolo3/export\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-UaGkGU6kOH",
        "colab_type": "code",
        "outputId": "50e5896f-0a7f-4367-e22d-867ad46c520b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_annotations.txt\n",
            "ch_00000.jpg.rf.3a408a1931e4a5721caada84d50ca3cb.jpg\n",
            "ch_00000.jpg.rf.86aede3bcf9d2e81bf05c91f453f024f.jpg\n",
            "ch_000010.jpg.rf.05e8c28282cc39a1f746ab3b02411990.jpg\n",
            "ch_000010.jpg.rf.7a067e8b9f1be408ea9d0732eaddd0aa.jpg\n",
            "ch_000011.jpg.rf.9856f4fc2275e5e06dcada930b26153f.jpg\n",
            "ch_000011.jpg.rf.a2f76cf2eca5af767c7a9a69ee1c98e2.jpg\n",
            "ch_000012.jpg.rf.d25a8aec1de84d98641462ff7bdf9e6d.jpg\n",
            "ch_000012.jpg.rf.fdb4da6ffa8606dcd751d6a26ef188e9.jpg\n",
            "ch_000013.jpg.rf.8af38c5c7d6d3d3385b9f59e72ccd6a1.jpg\n",
            "ch_000013.jpg.rf.faa4ba4c95b6f8446b3ccb94e3d83aa2.jpg\n",
            "ch_000014.jpg.rf.8d63bf81f69b73194151f6133ab26d20.jpg\n",
            "ch_000014.jpg.rf.ec6f40238a0978f2059aeb4de8f719e3.jpg\n",
            "ch_000015.jpg.rf.89e135a874bbcd5dfb73ba2f0f7c4eb4.jpg\n",
            "ch_000015.jpg.rf.f78ace876b15edfafa080a484d76cd9d.jpg\n",
            "ch_000016.jpg.rf.389923bee126feffd6b410b09aac0299.jpg\n",
            "ch_000016.jpg.rf.6b8e1c9a1b4ad0a67ba78bdef3f3ed6f.jpg\n",
            "ch_000017.jpg.rf.129d7ff1929cc1630165ccfa18715eaa.jpg\n",
            "ch_000017.jpg.rf.1b68f12cdbacc521c0df4c447f1606b7.jpg\n",
            "ch_000018.jpg.rf.a0cbb237d291dd9f75244dfab892857c.jpg\n",
            "ch_000018.jpg.rf.b20286a4c4d7e4f7849c4c439741ee1e.jpg\n",
            "ch_00001.jpg.rf.619340a0f891485382e0f97a060423a4.jpg\n",
            "ch_00001.jpg.rf.f22d7270f658f07bb23bcca977be3cdc.jpg\n",
            "ch_00002.jpg.rf.7169498bdf598df2f017be6432c20df4.jpg\n",
            "ch_00002.jpg.rf.ada2d2468fca43d0747e9bf7257b3b5b.jpg\n",
            "ch_00003.jpg.rf.1bcda9516e7fd0f9fefd2fa24afd000c.jpg\n",
            "ch_00003.jpg.rf.d618eb06a83cfbbfc982812326a552dc.jpg\n",
            "ch_00004.jpg.rf.3d8e2b97b86eb424e8d6dfea52e4b536.jpg\n",
            "ch_00004.jpg.rf.7dfd834640355c34a23bdd709fdac44d.jpg\n",
            "ch_00005.jpg.rf.356bee9aa07bd5ce14285ca198f4da94.jpg\n",
            "ch_00005.jpg.rf.c1383eb5ecc8e16026ab2f404dc0ed98.jpg\n",
            "ch_00006.jpg.rf.54e009367e7d4c95497c95b804b28fc3.jpg\n",
            "ch_00006.jpg.rf.617743ab1df4cbb68c5dd55254c7bdf3.jpg\n",
            "ch_00007.jpg.rf.066fc187e236517b2de6a24c3ed28f3b.jpg\n",
            "ch_00007.jpg.rf.90442f7d29ec708f6c1093f038829667.jpg\n",
            "ch_00008.jpg.rf.0e30446348fce76136ec0567bf3f28d5.jpg\n",
            "ch_00008.jpg.rf.f5f7cda23e8320b1224befcd0c91f3e5.jpg\n",
            "ch_00009.jpg.rf.2ae90e1c5b0de4436c4e0ae0ae3834e3.jpg\n",
            "ch_00009.jpg.rf.3676e9543ad5fd41fb27febcb32c46da.jpg\n",
            "ch_00019.jpg.rf.42fc004996aa563875f2d3a4756b37af.jpg\n",
            "ch_00019.jpg.rf.7e70d70127c55a3c7505c1a6db0fac7b.jpg\n",
            "ch_00020.jpg.rf.34a2e52bf8141639272c4cc8f777da5f.jpg\n",
            "ch_00020.jpg.rf.dc58edbab4edfa565f8b478f0ea83a55.jpg\n",
            "ch_00021.jpg.rf.78d41ae01b967dfa1115405c160ac3b1.jpg\n",
            "ch_00021.jpg.rf.82203afd3e75a6631c2e94a635797415.jpg\n",
            "_classes.txt\n",
            "de_00000.jpg.rf.3448473fa4a76b3e06b89f498b36477b.jpg\n",
            "de_00000.jpg.rf.f00db57bed78cdeb4805062e8584be94.jpg\n",
            "de_00001.jpg.rf.6b8fc96239144596a1bfaa52590c14fb.jpg\n",
            "de_00001.jpg.rf.9860ace2801f2902daf38d56c30ebb99.jpg\n",
            "de_00002.jpg.rf.49fe8e3e5179c7c2c1fa179cac555b04.jpg\n",
            "de_00002.jpg.rf.ce70f94e188ee7fbc9310a947d5d2d10.jpg\n",
            "de_00003.jpg.rf.9b0bb3c0ebcee7e5ecdc9defaea06a08.jpg\n",
            "de_00003.jpg.rf.afab645175e4812088dca1389dd35066.jpg\n",
            "de_00004.jpg.rf.31a284e8c2c0a7af0caa98a0fd2d5da4.jpg\n",
            "de_00004.jpg.rf.759b26e97d72e71d7832dc106d569bae.jpg\n",
            "de_00005.jpg.rf.303665fd06a407fcd21eae54f55c3dd4.jpg\n",
            "de_00005.jpg.rf.68e0b103b9db4ab9e91c5603a1dad26a.jpg\n",
            "de_00006.jpg.rf.5c4b7635f640772f59438891a72a6d33.jpg\n",
            "de_00006.jpg.rf.9ea4cd4ca650d67c45c9957c0f0ed5cf.jpg\n",
            "de_00007.jpg.rf.3cbf53013a2dbaebd5502b3edf5c4c16.jpg\n",
            "de_00007.jpg.rf.75200ab25ec597abf07b76d48dedb237.jpg\n",
            "de_00008.jpg.rf.a2e50661efdbc16273ae436e4c37d268.jpg\n",
            "de_00008.jpg.rf.b83061f1f9fcb81ebfc824929e15c162.jpg\n",
            "de_00009.jpg.rf.2f9e9d5f6b4d5197d1701997a5f4b10e.jpg\n",
            "de_00009.jpg.rf.bef9fb521b64136e9147671b42a8b35c.jpg\n",
            "de_00010.jpg.rf.08fe7088c5175cecbba205c31f8ac38e.jpg\n",
            "de_00010.jpg.rf.7917a8d74c2497a6f081e3c5fd41a056.jpg\n",
            "de_00011.jpg.rf.73d69d14308dc890ee252aa0a7ca17ce.jpg\n",
            "de_00011.jpg.rf.f3b9759f4388caa7bce574c482e74b0a.jpg\n",
            "de_00012.jpg.rf.32a823a6ee50446d7f3285dbbe88fcd1.jpg\n",
            "de_00012.jpg.rf.eb11b398a9cd8796ed5ee93c83098979.jpg\n",
            "de_00013.jpg.rf.81096463dcdb882f0060e8fc2490e7ca.jpg\n",
            "de_00013.jpg.rf.b20c5163ed5b35e4c96dca3ab849e628.jpg\n",
            "de_00014.jpg.rf.051c039d80271b1751b87aff33edceed.jpg\n",
            "de_00014.jpg.rf.87a12f55a50b4b3636ac0874e5384411.jpg\n",
            "de_00015.jpg.rf.31caabf1141f31f20c643f2c8f3db517.jpg\n",
            "de_00015.jpg.rf.9fc7949bb93d30840d0e65d48cf8c143.jpg\n",
            "de_00016.jpg.rf.8eff6151adc63c3a4bd23b1908ef912c.jpg\n",
            "de_00016.jpg.rf.cc9f86e643b728f64aa401c68320b8ad.jpg\n",
            "de_00017.jpg.rf.b15546bf29ed3e0fb5b692f446842c1f.jpg\n",
            "de_00017.jpg.rf.fa958d7d54aab37ce5992de5017a1953.jpg\n",
            "de_00018.jpg.rf.0edcad9fae8d601395bac9f6af94d10c.jpg\n",
            "de_00018.jpg.rf.f4f0f59fedd0f5d3d22dbfafb73e144b.jpg\n",
            "es_00000.jpg.rf.7ad53ac639651904947b9774f6190291.jpg\n",
            "es_00000.jpg.rf.8ecd4beeb207f24ebe3c71ef4e208a42.jpg\n",
            "es_00001.jpg.rf.b6d50ae67796c53e355ef389b048468d.jpg\n",
            "es_00001.jpg.rf.d163405daa41ae749437990b58d0df39.jpg\n",
            "es_00002.jpg.rf.798e9ef20fbb246406755677329488f4.jpg\n",
            "es_00002.jpg.rf.e2349be517edb678f20606b4d23fc726.jpg\n",
            "es_00003.jpg.rf.233df977c7c83ddffc851e478d5f3e3f.jpg\n",
            "es_00003.jpg.rf.5612006372b0c2bb3f85c0191f372867.jpg\n",
            "es_00004.jpg.rf.1f7d2afd01479da137e19d17d287bb97.jpg\n",
            "es_00004.jpg.rf.750873f276df3b6a4aa16ca0f49baa29.jpg\n",
            "es_00005.jpg.rf.0fad5024243923b073bedf4756908cd9.jpg\n",
            "es_00005.jpg.rf.8b634d843290f6eaf74dda6af2f3b6a1.jpg\n",
            "es_00006.jpg.rf.01762b3ea80d3d1ea50a18a8398bb943.jpg\n",
            "es_00006.jpg.rf.bcfb088064b1a70d4073fe508b07e0d6.jpg\n",
            "es_00007.jpg.rf.04506fe9483c61a2734ef6a87ecf8941.jpg\n",
            "es_00007.jpg.rf.cb1a65e14657429b99efca6063009566.jpg\n",
            "es_00008.jpg.rf.a908cd878bb0fd597de7a21e7e91b3ec.jpg\n",
            "es_00008.jpg.rf.ba97e93e4983425a60ef7b206197d375.jpg\n",
            "es_00009.jpg.rf.282d23d90f738b534fce016b49ce8f56.jpg\n",
            "es_00009.jpg.rf.5a1fd78945f740cf4a1ae8db4f8cae46.jpg\n",
            "es_00010.jpg.rf.e34e5db54c7c8ebf18f0a27cd0a0a7fd.jpg\n",
            "es_00010.jpg.rf.ee04e7e83d8e8528875a4c194bf33897.jpg\n",
            "es_00011.jpg.rf.08cadef83c16bed6174f7d9c1cd0acf0.jpg\n",
            "es_00011.jpg.rf.da72fe28192c54050a9f3afd0f4b6f13.jpg\n",
            "es_00012.jpg.rf.ed9317c60a0646818e7a7c1a4906838a.jpg\n",
            "es_00012.jpg.rf.f2127a7f62496343d048923f90d2ce03.jpg\n",
            "es_00013.jpg.rf.6723627b9c88373235439f76f703c997.jpg\n",
            "es_00013.jpg.rf.a593cd36bbfadf19a4f65df8076c3cbf.jpg\n",
            "es_00014.jpg.rf.2fc917dc01f2f893d5b53ca9052978a1.jpg\n",
            "es_00014.jpg.rf.649d616f8d2958826b22423279c72780.jpg\n",
            "es_00015.jpg.rf.4e8ec3eafb0d81eb863779821b340b7d.jpg\n",
            "es_00015.jpg.rf.dfccd3cd0496a41f6ed2512514482ec5.jpg\n",
            "es_00016.jpg.rf.195c007d148c0ecbe1272ac327975549.jpg\n",
            "es_00016.jpg.rf.bd1afd7019e28fb0b0e9fa7974c9d812.jpg\n",
            "es_00017.jpg.rf.6802ba3a42c61f9681409656a35cc310.jpg\n",
            "es_00017.jpg.rf.98cd25063e8baa7fe3b1fd552c45fb18.jpg\n",
            "es_00018.jpg.rf.8c86228b657f8580ddd2df2fb95dea21.jpg\n",
            "es_00018.jpg.rf.b7d0d425ef24d290e74d75eebc2a9504.jpg\n",
            "es_00019.jpg.rf.62c72012aa22dad784c6abc058939de0.jpg\n",
            "es_00019.jpg.rf.e57ed7df62262f405e77a1a98938f955.jpg\n",
            "es_00020.jpg.rf.31e9eb8c9e94a966dcc48cf4d5e20304.jpg\n",
            "es_00020.jpg.rf.54eedb612eb6b00ee6b0add9b52a7fae.jpg\n",
            "es_00021.jpg.rf.64a52077e6df3f1514f24d4f8f043d5c.jpg\n",
            "es_00021.jpg.rf.702ad0e2882c61b4db371c052bd4da9a.jpg\n",
            "es_00022.jpg.rf.e426120a5e88fd31bbe2dbcdb2529c81.jpg\n",
            "es_00022.jpg.rf.ec84510277f0b44c4931bb526a2a6b5d.jpg\n",
            "es_00023.jpg.rf.11fcc5cbfdce683e96694dc2e1a94f64.jpg\n",
            "es_00023.jpg.rf.557ff36d82cc163a42a781f2c7d34942.jpg\n",
            "es_00024.jpg.rf.038453c3c3fe6c4dd05ef9c51f662e32.jpg\n",
            "es_00024.jpg.rf.9729ac86c102d55bda5e6f8ccb603ad0.jpg\n",
            "es_00025.jpg.rf.0652fe2a16f336088f40f542b7e12521.jpg\n",
            "es_00025.jpg.rf.ac57a12aedaee36e62c5286ca705cb7b.jpg\n",
            "fr_00000.jpg.rf.a72aa9d0477b9436fc0b47b16dba5894.jpg\n",
            "fr_00000.jpg.rf.c7ef6714a672b53e79f839c65209baf0.jpg\n",
            "fr_00001.jpg.rf.09400b0497c4261ee6b7789ccd16a180.jpg\n",
            "fr_00001.jpg.rf.c455b718bea7a640f586a91bde2e83ba.jpg\n",
            "fr_00002.jpg.rf.32c47d0b1fe01971724b1ca6448350a8.jpg\n",
            "fr_00002.jpg.rf.62b8a1fede967c495c8ed938ad9b4f67.jpg\n",
            "fr_00003.jpg.rf.2b7f7200cdb0043a4ee35a60da05ff0c.jpg\n",
            "fr_00003.jpg.rf.d4ff49cd9f8805d497983ccb7936465f.jpg\n",
            "fr_00004.jpg.rf.00cf20797473c6ef6d881c3c11ce7f70.jpg\n",
            "fr_00004.jpg.rf.44f7b61c2495fc4b7e0f650db6bdb996.jpg\n",
            "fr_00005.jpg.rf.bfa531d6eaa242f5784fa74818ff63b4.jpg\n",
            "fr_00005.jpg.rf.c55a71457a1361ea024dffa25ea3731b.jpg\n",
            "fr_00006.jpg.rf.9d8de1e1aa45ae8e3f4ed94cf7b02521.jpg\n",
            "fr_00006.jpg.rf.ffd58b316357a3d0957b8116046c2463.jpg\n",
            "fr_00007.jpg.rf.50ec4d6b8d7b5c4091af82edbd1f5710.jpg\n",
            "fr_00007.jpg.rf.d6d7de0a196f63f2244a84d732a2c2af.jpg\n",
            "fr_00008.jpg.rf.5a371281c7489acf7e12d1f0fd309523.jpg\n",
            "fr_00008.jpg.rf.c02bbb808a2e7f51de3701335c9b31cc.jpg\n",
            "fr_00009.jpg.rf.c2c13bbc8e5f26a5b659e0f7815c7047.jpg\n",
            "fr_00009.jpg.rf.f51425f2bf3d18070b572daee2004dd8.jpg\n",
            "fr_00010.jpg.rf.069bb973a6f2a8ba7be2ce196de820e6.jpg\n",
            "fr_00010.jpg.rf.fb3b20a611c7f527697e817e1f53f3d1.jpg\n",
            "fr_00011.jpg.rf.1e5cb872cb6cd59e3fdbbb117841ed74.jpg\n",
            "fr_00011.jpg.rf.d763a9071cdbbf10d1349086f0e0427b.jpg\n",
            "fr_00012.jpg.rf.4ee55e8ff4e8c6d9dbd79075fbc1b10b.jpg\n",
            "fr_00012.jpg.rf.f79c53de5040566632f2ac1747b5a28b.jpg\n",
            "fr_00013.jpg.rf.8127f7b63bc6c719baec95ad23f0628f.jpg\n",
            "fr_00013.jpg.rf.f698f4c666aecb7ec9ac3138065d1dee.jpg\n",
            "gr_00000.jpg.rf.d5cb01cf64bbff61108a800622d77248.jpg\n",
            "gr_00000.jpg.rf.ee8e57b2f81b2f466c3bbcf094327853.jpg\n",
            "gr_00001.jpg.rf.a4c52f9a1841192d071d97d301dffc5e.jpg\n",
            "gr_00001.jpg.rf.f5e8cd4ee91b112064adf93d6ca59e7a.jpg\n",
            "gr_00002.jpg.rf.150f624a29d296dca6faaa64e93143a3.jpg\n",
            "gr_00002.jpg.rf.d9ab6d4b87c2cf54d93f6474406bc83a.jpg\n",
            "gr_00003.jpg.rf.7b9e148bc76967d180ece5a2f70b4cc7.jpg\n",
            "gr_00003.jpg.rf.9abe6962bab00b2345665517a11a8b5e.jpg\n",
            "gr_00004.jpg.rf.438a9801e81d43b499cd5dc421c13b27.jpg\n",
            "gr_00004.jpg.rf.fb824e2d95952bc369e43150288fe60e.jpg\n",
            "gr_00005.jpg.rf.66d69c3ba94e16bec1b4d9371e78cb99.jpg\n",
            "gr_00005.jpg.rf.900a5768b046c3dac32367547ef89e46.jpg\n",
            "gr_00006.jpg.rf.8405927c6eae856fc37e68b325ab2233.jpg\n",
            "gr_00006.jpg.rf.f946131fc4a245d31386f3096a48014f.jpg\n",
            "gr_00007.jpg.rf.0d9107e4cd565a1cc484679e507f6505.jpg\n",
            "gr_00007.jpg.rf.ebde6ca2adad7afc51968eb82e8d2299.jpg\n",
            "gr_00008.jpg.rf.556d1e6dc057f6d758608b0ebeeeb480.jpg\n",
            "gr_00008.jpg.rf.9f06f46201038468c2617e85140fc8de.jpg\n",
            "gr_00009.jpg.rf.c4cf25e2da336bf50a8a288200800d04.jpg\n",
            "gr_00009.jpg.rf.f6f8a743b1e366242ad98f66baef84a5.jpg\n",
            "gr_00010.jpg.rf.0125e1d9d2f16e8c8cc5aeecc2d64ca4.jpg\n",
            "gr_00010.jpg.rf.c263a7c644f5746473339019d1d49f4a.jpg\n",
            "gr_00011.jpg.rf.34087e3d2ac4e00c83b75e0801d781d7.jpg\n",
            "gr_00011.jpg.rf.74d609d1e07d9376e9aafef1061b9a86.jpg\n",
            "gr_00012.jpg.rf.3e6a7c3bc78e6dca286249a11f299378.jpg\n",
            "gr_00012.jpg.rf.7d27b2a3a6ce00aea67fe35b0477eb59.jpg\n",
            "gr_00013.jpg.rf.9bb859cea19f23a798d5ab78f92bd75f.jpg\n",
            "gr_00013.jpg.rf.cb2fd2b1ae38014e455e37478992cb56.jpg\n",
            "gr_00014.jpg.rf.158600343ed0a8441b55cef2305d0ec9.jpg\n",
            "gr_00014.jpg.rf.554e5662fbe95c4bc0bec7f588d764fe.jpg\n",
            "gr_00015.jpg.rf.8a0cdeb3e9a6a88c0248c5d67a101f9f.jpg\n",
            "gr_00015.jpg.rf.c67e6b89c9044963a12ad21355595140.jpg\n",
            "gr_00016.jpg.rf.b293451355d0d0e6c740b63a69023c7d.jpg\n",
            "gr_00016.jpg.rf.daa2a843a0ca99f76fe2b503c5dffd79.jpg\n",
            "gr_00017.jpg.rf.728a4f4fbddcedea9b82925b95595929.jpg\n",
            "gr_00017.jpg.rf.92575aba13a94575ee54c40be34f616a.jpg\n",
            "gr_00018.jpg.rf.08f9b5a1e30c4dbdfdb5ef86c2e3f23e.jpg\n",
            "gr_00018.jpg.rf.971353df061e65dc2852f0ff27b9a793.jpg\n",
            "gr_00019.jpg.rf.cf10c12f7f306ed5f6f875a9855e4012.jpg\n",
            "gr_00019.jpg.rf.daf16e8dea0aef67a1f72f29aa33b50b.jpg\n",
            "hu_00000.jpg.rf.3e63f649b84e331b332fe49398a54d03.jpg\n",
            "hu_00000.jpg.rf.da408b5614a5a07dfdbbf7afee10d4f3.jpg\n",
            "hu_00001.jpg.rf.728bcc706d706b732fd9eb8b20214be9.jpg\n",
            "hu_00001.jpg.rf.ccd4a23aa5d1bd7b7f827bad82b0e740.jpg\n",
            "hu_00002.jpg.rf.320ed34b24f25b0314683e55f599eecc.jpg\n",
            "hu_00002.jpg.rf.ccfafc2e3d6573bd4d5f9353395cbacf.jpg\n",
            "hu_00003.jpg.rf.428ae76d00c5545a351245ecbdb9cea3.jpg\n",
            "hu_00003.jpg.rf.9cf4a737b10d694de421c99dd967f165.jpg\n",
            "hu_00004.jpg.rf.1cf47a5ff2acd9fce4ed89e2444fc000.jpg\n",
            "hu_00004.jpg.rf.54951122dc9dbae564445072ef608fc5.jpg\n",
            "hu_00005.jpg.rf.4e9ccbcdd8a222751777dd9319106c6a.jpg\n",
            "hu_00005.jpg.rf.f3709ed59787f1d1cefa5b1a619d8552.jpg\n",
            "hu_00006.jpg.rf.bbbaa0274e39211bbdc844d87a401a0d.jpg\n",
            "hu_00006.jpg.rf.cedb720729c2a2478a452cdc3e0f49f2.jpg\n",
            "hu_00007.jpg.rf.51181f8c43320c7977cd7a91342c5c4e.jpg\n",
            "hu_00007.jpg.rf.6ca20d896c92eb63832053926067c4e8.jpg\n",
            "hu_00008.jpg.rf.5bf39d6145ce9242f96e8251a35a4afa.jpg\n",
            "hu_00008.jpg.rf.f48c9340dfe0865836ebf0c60666e615.jpg\n",
            "hu_00010.jpg.rf.4528519852d92d2b342e191526416248.jpg\n",
            "hu_00010.jpg.rf.607687716f75880c7a53387cea663186.jpg\n",
            "hu_00011.jpg.rf.79d392120c1675fc91c768d1d844f110.jpg\n",
            "hu_00011.jpg.rf.ea0515d0a35d3d073838c1c6103c011a.jpg\n",
            "hu_00012.jpg.rf.3c0d71211c89c7f138a832fba33ede47.jpg\n",
            "hu_00012.jpg.rf.aed997b2f16261c5700dff0c5c8a39e3.jpg\n",
            "hu_00013.jpg.rf.76d70c9267cbfd194e4b2abfb739e209.jpg\n",
            "hu_00013.jpg.rf.a5acd2a25811fcc7723547b9d6d276e5.jpg\n",
            "hu_00015.jpg.rf.2024aeeea0d84dca750d4adc8c8c637d.jpg\n",
            "hu_00015.jpg.rf.c1af265fb277c32d311f330f5be257ef.jpg\n",
            "hu_00016.jpg.rf.6166f76d910d47a471684d102388105f.jpg\n",
            "hu_00016.jpg.rf.96b12c92b0eaa70e5e81d40687a6eb7b.jpg\n",
            "me_00000.jpg.rf.5b49d8363b1d722f9e851114b6af1294.jpg\n",
            "me_00000.jpg.rf.bd710d2668f56fd162f862dbfdafad6e.jpg\n",
            "me_00001.jpg.rf.079cde12e7b6d606e47c70a1e16d7c8f.jpg\n",
            "me_00001.jpg.rf.3bd7fdb4e928616f4fc9af4d6aa01fa1.jpg\n",
            "me_00002.jpg.rf.9cd48244747570c0e7ba3c5b82d15e99.jpg\n",
            "me_00002.jpg.rf.c1791511d9911ed099c418da70729ddb.jpg\n",
            "me_00003.jpg.rf.591c9c2a47f6884475184ac28f7909e1.jpg\n",
            "me_00003.jpg.rf.f8bc6a45e30bf79024fb3e518f636ed0.jpg\n",
            "me_00004.jpg.rf.669b5b71d145db7029a4ad4c75a96f4f.jpg\n",
            "me_00004.jpg.rf.9bed0e01fb221402d59ee3872a859faf.jpg\n",
            "me_00005.jpg.rf.6d78d33185e036d931a532912a60fa8c.jpg\n",
            "me_00005.jpg.rf.fb25d1cae228a5f8b9082eb293ac66d9.jpg\n",
            "me_00006.jpg.rf.384a63ad66a57ae5be1ccf8f593de222.jpg\n",
            "me_00006.jpg.rf.fc47a89da6dacd2a5e1e1c37420655fc.jpg\n",
            "me_00007.jpg.rf.053395b7ceb0fa5a43988ace2bd100ae.jpg\n",
            "me_00007.jpg.rf.bc3a925da8a2756951863ca3f2a47e59.jpg\n",
            "me_00008.jpg.rf.38b416930355ae0a199ec6372b8b4bae.jpg\n",
            "me_00008.jpg.rf.a64bf8a848354843288569c6dee8a783.jpg\n",
            "me_00009.jpg.rf.c11cf7c4933a25c958ab436a3d436088.jpg\n",
            "me_00009.jpg.rf.cad6cc8a72629cee90cafb078de3f2b9.jpg\n",
            "me_00010.jpg.rf.23500ba05c3649629e02185a436c65e5.jpg\n",
            "me_00010.jpg.rf.a349f17ad152f5fc5a47d3b8a2439196.jpg\n",
            "me_00011.jpg.rf.7f69818805e1f13f1ecb2a08f38e099c.jpg\n",
            "me_00011.jpg.rf.a0113e48820b5b92dee9a5c620dada71.jpg\n",
            "me_00012.jpg.rf.8fe99cd9e71b3ef858ac2ef40a96c2de.jpg\n",
            "me_00012.jpg.rf.9422e0f868bbc1a540ca2532585e0aad.jpg\n",
            "me_00013.jpg.rf.1643c06bb0984006c53ac01266b74fd5.jpg\n",
            "me_00013.jpg.rf.d3a655453c8ae545a8bcf9c5f700ad2a.jpg\n",
            "me_00014.jpg.rf.37337144b9d6e778882e30685c1ab8f9.jpg\n",
            "me_00014.jpg.rf.ceecce07025da89383ce4b303e42282b.jpg\n",
            "me_00015.jpg.rf.18d2f2fa3aaf06cf8e85f6fc34b45c46.jpg\n",
            "me_00015.jpg.rf.db391f1919e8b9802653ba829f342e04.jpg\n",
            "pt_00000.jpg.rf.1b3167a38dce0a33e998efbaaf6de28a.jpg\n",
            "pt_00000.jpg.rf.e54bb0a9d1b8bfd3f981e10470f33d2b.jpg\n",
            "pt_00001.jpg.rf.966691a7319c957dd63e6939363c88d4.jpg\n",
            "pt_00001.jpg.rf.a799954727943d3ba4131fd627154030.jpg\n",
            "pt_00002.jpg.rf.5eb3493094c9a98ff2503ccb89a96bac.jpg\n",
            "pt_00002.jpg.rf.b633af26a1b33a8276632faeab1176ce.jpg\n",
            "pt_00003.jpg.rf.b476adbad7cb2e914da9baa5007d5884.jpg\n",
            "pt_00003.jpg.rf.d9113545355a40f7ea02bd4d02063ecd.jpg\n",
            "pt_00004.jpg.rf.19c7c758028fdacda7154b6634626e3e.jpg\n",
            "pt_00004.jpg.rf.6109dfaf483e642be978a1f50db40e23.jpg\n",
            "pt_00005.jpg.rf.1aa142b8179dccb5991d97a4ca8d79d5.jpg\n",
            "pt_00005.jpg.rf.9e639382058fc4cd83020bd3e6441bcd.jpg\n",
            "pt_00006.jpg.rf.12cb3f1a01b2a6c36894735c25848c47.jpg\n",
            "pt_00006.jpg.rf.e56788fa3acfa1244b9ccc1b26769e5a.jpg\n",
            "pt_00007.jpg.rf.93afa937df4ce8bd145eb661ea144323.jpg\n",
            "pt_00007.jpg.rf.fd7659e178e9166ce33ee2c30c4bc81e.jpg\n",
            "pt_00008.jpg.rf.099101bb0e3111075612617d07f19fa8.jpg\n",
            "pt_00008.jpg.rf.1f6466f356f2b88b9f9c63761d0d7bcb.jpg\n",
            "pt_00009.jpg.rf.30a082eff0e20961593fe54f92fdad21.jpg\n",
            "pt_00009.jpg.rf.82cbc13c70e3abb77a95b5567fc026a3.jpg\n",
            "pt_00010.jpg.rf.87d89229db6ef8f6201faf18a500e023.jpg\n",
            "pt_00010.jpg.rf.95b34a89122f884e8e069c61a3a707e6.jpg\n",
            "pt_00011.jpg.rf.34a39d46b3be0500645c7047fd470770.jpg\n",
            "pt_00011.jpg.rf.76c1353837b187cf4cf85f006f2e09a4.jpg\n",
            "pt_00012.jpg.rf.33d1e02c740e181662a542ad05402040.jpg\n",
            "pt_00012.jpg.rf.bf2ed226095d71063c2af1e3e824c63b.jpg\n",
            "pt_00013.jpg.rf.06f99ace0c399b4145e9e27f2615d1c5.jpg\n",
            "pt_00013.jpg.rf.bafaee71a5307e9a1c0bccd940859517.jpg\n",
            "pt_00014.jpg.rf.628ed5ad0d2b78c8eccc52d9c9de8358.jpg\n",
            "pt_00014.jpg.rf.f11307dc1a3b35619d2badffb7932e90.jpg\n",
            "pt_00015.jpg.rf.2835271fdce493dafaddf9d0c2cbae2f.jpg\n",
            "pt_00015.jpg.rf.4525ce0abd32d19c2e3bc321f4b3dd30.jpg\n",
            "pt_00016.jpg.rf.78b9a4c0468c7ae6738e17f3f1142522.jpg\n",
            "pt_00016.jpg.rf.f9354e6e3daca2ad33fc50efb4b6f37f.jpg\n",
            "pt_00017.jpg.rf.630478ab2e29cd2d0cabf323e90f9078.jpg\n",
            "pt_00017.jpg.rf.d0f3f8863514bd4a7d1b44370d152d38.jpg\n",
            "pt_00018.jpg.rf.07c198490d2d6750d92931a17294f4bb.jpg\n",
            "pt_00018.jpg.rf.7a48b50f2dbc2ec9d7c5141bd370742b.jpg\n",
            "pt_00019.jpg.rf.9ddd1613bdecfae16cd7cb38bed6711e.jpg\n",
            "pt_00019.jpg.rf.bb1d427c38a9a523695cd9706a5a4b3f.jpg\n",
            "rs_00000.jpg.rf.5d131dff8a80dab17a9fb81f4dc23fc0.jpg\n",
            "rs_00000.jpg.rf.a7f7e7dd2e22d5fca1e17516e1ff005b.jpg\n",
            "rs_00001.jpg.rf.3b434e92c8ca865bd55d65c335a2c5ed.jpg\n",
            "rs_00001.jpg.rf.618659509637b8acbeb65b33dd5a56cf.jpg\n",
            "rs_00002.jpg.rf.0a0342e57b8f44f5e1e7067d4c46722a.jpg\n",
            "rs_00002.jpg.rf.8eb2c15620b938ecb8ca978f74018c02.jpg\n",
            "rs_00003.jpg.rf.351e94dd996c00fd101ad3c62cdca4c3.jpg\n",
            "rs_00003.jpg.rf.9c136f72dd4bcae5109d8b22a672d076.jpg\n",
            "rs_00004.jpg.rf.18ff27f5758488112ff10a0cad6b9986.jpg\n",
            "rs_00004.jpg.rf.8695c573e0dbde9f9813fa7be47adda0.jpg\n",
            "rs_00005.jpg.rf.dd7944a02dc4720bb6f568199aa23d11.jpg\n",
            "rs_00005.jpg.rf.f7ca8176ec3fb4b2461b6360d2727dda.jpg\n",
            "rs_00006.jpg.rf.10254448501351d957b20786b5256b25.jpg\n",
            "rs_00006.jpg.rf.7518dc78ae1f2c961b12f49cda290967.jpg\n",
            "rs_00007.jpg.rf.4890b66bc58247580144777bc83da2ac.jpg\n",
            "rs_00007.jpg.rf.9e892c71330fd0598188690f145e06f5.jpg\n",
            "rs_00008.jpg.rf.99ed6634d3b6a9f4f32702bd5b6a7e45.jpg\n",
            "rs_00008.jpg.rf.cf075d67b4500fcad736b35f55bc3667.jpg\n",
            "rs_00009.jpg.rf.08e52e059e6c12312c8732b57fea2996.jpg\n",
            "rs_00009.jpg.rf.a0d479b1a46c1c46ab8dedfff82600ae.jpg\n",
            "rs_00010.jpg.rf.609ecf806c32842ac984e500eda72f9b.jpg\n",
            "rs_00010.jpg.rf.d42bec3672cb57db57174c266dd45f20.jpg\n",
            "rs_00011.jpg.rf.3493fecbde2c8f8aba80bea49f6d3c46.jpg\n",
            "rs_00011.jpg.rf.d1149470d49e9e2ace439d29c1d5c360.jpg\n",
            "rs_00012.jpg.rf.2934e1b08ced2909d7ed129ef0419a3f.jpg\n",
            "rs_00012.jpg.rf.b595ae9fd868fadcf81384ced0c4a55a.jpg\n",
            "rs_00013.jpg.rf.38a8153439cb30151e85a934d83c6d95.jpg\n",
            "rs_00013.jpg.rf.3d332d83d933f70422694e2f70817767.jpg\n",
            "rs_00014.jpg.rf.49da48002e313e7d0ecf1644b65e6a03.jpg\n",
            "rs_00014.jpg.rf.d7d720a99a6fb9faf271d8e829cbe582.jpg\n",
            "rs_00015.jpg.rf.6670edfd704ea291bd7e5da1a7337511.jpg\n",
            "rs_00015.jpg.rf.7894990bdd474c0b4916bb5602e155df.jpg\n",
            "rs_00016.jpg.rf.4c1c2defdea93b0030ae112234056169.jpg\n",
            "rs_00016.jpg.rf.9b6d4656480f8bc12e4c17c08539d82b.jpg\n",
            "rs_00017.jpg.rf.10b4222e87aa05aa0c57830a98a9d9ef.jpg\n",
            "rs_00017.jpg.rf.91dc95b9eb50945d537eae3fc28099f7.jpg\n",
            "rs_00018.jpg.rf.a705f8300dbf49ff429f8496346fa1d1.jpg\n",
            "rs_00018.jpg.rf.f0fac0ebefb71d8a0e41c921773d6c79.jpg\n",
            "rs_00019.jpg.rf.45c34a8684e85f379c62e2ef43ebb0a7.jpg\n",
            "rs_00019.jpg.rf.9eb76c0b8fd7b558734cb9e91d6c25bf.jpg\n",
            "rs_00020.jpg.rf.efa54fe2a83ed7920810ef959625a979.jpg\n",
            "rs_00020.jpg.rf.fb3c1b8c3e869eb81f1e33482d84cc00.jpg\n",
            "rs_00021.jpg.rf.ad28fcdd4a8882a25d1e483ac326ef75.jpg\n",
            "rs_00021.jpg.rf.e507fdcc67cc26074bc4c8892097dceb.jpg\n",
            "rs_00022.jpg.rf.18a96de4400029e341ecdb2f8bcf7722.jpg\n",
            "rs_00022.jpg.rf.e75e55d6303f92d6a0ee9fa6657eea3e.jpg\n",
            "rs_00023.jpg.rf.5527b5625bd4f6e78fef95f7a51acece.jpg\n",
            "rs_00023.jpg.rf.82877aba78c30838020aa42063d3f746.jpg\n",
            "rs_00024.jpg.rf.07917acbb8b031cddf70f237bc6961f1.jpg\n",
            "rs_00024.jpg.rf.e2a0ab853bdb0dcaf036697953b64b60.jpg\n",
            "rs_00025.jpg.rf.0ebd22318326599314c0778ac6d6c43a.jpg\n",
            "rs_00025.jpg.rf.54b1e1a642ffb1fe4e2c03080b92ffd7.jpg\n",
            "rs_00026.jpg.rf.08e0b63805f6cc147662b859e4797ac0.jpg\n",
            "rs_00026.jpg.rf.4169305d3425bf1c82e386fd75ea4054.jpg\n",
            "rs_00027.jpg.rf.50b4bad6d992f0a91e1e1a84e158f0a6.jpg\n",
            "rs_00027.jpg.rf.9bc17496983795fec781f0af6c92f2c7.jpg\n",
            "rs_00028.jpg.rf.c16b1ec4ca75fc8d796dcd928915aa6c.jpg\n",
            "rs_00028.jpg.rf.fbbf0c591718f85e9f9ec2b7433ad981.jpg\n",
            "rs_00029.jpg.rf.55b9f55a19a5680f04671522ac12486d.jpg\n",
            "rs_00029.jpg.rf.6cc3b03861410ac841400a8267eece0a.jpg\n",
            "rs_00030.jpg.rf.67349b7bb3f6402105676a751a084388.jpg\n",
            "rs_00030.jpg.rf.8ebfb9022100e4160ebd62eac95fa378.jpg\n",
            "rs_00031.jpg.rf.c1b97f4391c617f7ec798f9f68743791.jpg\n",
            "rs_00031.jpg.rf.e2135613e46ca8d9b7bd675bb9db4644.jpg\n",
            "rs_00032.jpg.rf.91e43a154dcb4bd62fb985d765504f85.jpg\n",
            "rs_00032.jpg.rf.f1138cdabc7d2d357cabd4f1c0e15dd4.jpg\n",
            "rs_00033.jpg.rf.71a1da23b2290ebc8f6377fcca28979a.jpg\n",
            "rs_00033.jpg.rf.8bb373f0f8ebd716add7d0ed81f91ee8.jpg\n",
            "rs_00034.jpg.rf.5816075f55a1a3b45bbca2bfd0e9b6ad.jpg\n",
            "rs_00034.jpg.rf.bd7af706b1ce144f13698e819adf8e3c.jpg\n",
            "rs_00035.jpg.rf.64d95bdcc3d7ec76441a3fcd3ffc0e92.jpg\n",
            "rs_00035.jpg.rf.a480c46ea22ea9218fcbb2040ce353ed.jpg\n",
            "rs_00036.jpg.rf.020c0c36d1d231608e4a102ea7b290b6.jpg\n",
            "rs_00036.jpg.rf.4e2178b58055d5693d941abc4fa122e8.jpg\n",
            "rs_00037.jpg.rf.51f5b639736273833cdb2c0bb67ca8b7.jpg\n",
            "rs_00037.jpg.rf.c95c56edf449ac560554550e13836302.jpg\n",
            "rs_00038.jpg.rf.1953f4f30753091382d0c1fe0d87de52.jpg\n",
            "rs_00038.jpg.rf.3379b8282f5c3aba7707c07fc27f014a.jpg\n",
            "rs_00039.jpg.rf.3595fdb3e71b484d98b1f9295630990b.jpg\n",
            "rs_00039.jpg.rf.58f1ef9e46bbcb94d50b0fb236a7705f.jpg\n",
            "rs_00040.jpg.rf.5f5abd26e07a5b4600ce58770b80ddf1.jpg\n",
            "rs_00040.jpg.rf.fcdb0e566a53962f9319f755bd8e510e.jpg\n",
            "rs_00041.jpg.rf.04c43db05fdc7d4f9ad80c052048db3b.jpg\n",
            "rs_00041.jpg.rf.e28039df733822ed25af85c71914451b.jpg\n",
            "rs_00042.jpg.rf.5e08096a8228d75bd77e7d36a5a3162d.jpg\n",
            "rs_00042.jpg.rf.c069fc04666fa6208b5f09e31c65af55.jpg\n",
            "rs_00043.jpg.rf.3ba9996626ffd1ec29e6d324ecc5d5aa.jpg\n",
            "rs_00043.jpg.rf.f5295c61560178ec3bb5dc22fbaacc6b.jpg\n",
            "rs_00044.jpg.rf.2235f631beca436a26c1ef7e6c0e3007.jpg\n",
            "rs_00044.jpg.rf.eb62dc1c0ae5731184fe3820de4ff1ce.jpg\n",
            "rs_00045.jpg.rf.bbae68fe7d324d0c27c4efa5ecf29fc3.jpg\n",
            "rs_00045.jpg.rf.d6dd0e30aa332d92989fbed147779962.jpg\n",
            "rs_00046.jpg.rf.7ee5ac9fc84982312ff1cc8457faab9d.jpg\n",
            "rs_00046.jpg.rf.b3961b6673fe1f2fd5f8927d88ebd247.jpg\n",
            "rs_00047.jpg.rf.1417fd442f94006bdc2a8453a6837130.jpg\n",
            "rs_00047.jpg.rf.63653908d74000854034185993f2fc71.jpg\n",
            "rs_00048.jpg.rf.5be8a1106be57313d9282c3b94ec8259.jpg\n",
            "rs_00048.jpg.rf.7225ee0fc2b4d9728b485171ed615d73.jpg\n",
            "rs_00049.jpg.rf.24ba75a1ffd694d7c886a858e53e2615.jpg\n",
            "rs_00049.jpg.rf.3053143ccfa6205fa8c5678e9641e05d.jpg\n",
            "rs_00050.JPG.rf.10b42b589bbae7a399612a46daa360ca.jpg\n",
            "rs_00050.JPG.rf.47698ccfdca09a33d9fdc5f70924680d.jpg\n",
            "rs_00051.jpg.rf.5b8ebcf863e248eb167873cd5ce61d5a.jpg\n",
            "rs_00051.jpg.rf.ca7266be54f4d4f5060ecc851e846791.jpg\n",
            "rs_00052.jpg.rf.448a29a6e61a566cdb75f1d4ec06d71a.jpg\n",
            "rs_00052.jpg.rf.e8d059a33ae45f6343d94b7ca20f13bc.jpg\n",
            "si_00000.jpg.rf.43d7bf5fecd4ba6432187f3af84874a6.jpg\n",
            "si_00000.jpg.rf.b5672e58e8df057e638f29cfa345687b.jpg\n",
            "si_00001.jpg.rf.cb8779cc251496760db08dc6203e0f51.jpg\n",
            "si_00001.jpg.rf.d9093b12ebae244366933391de7e371d.jpg\n",
            "si_00002.jpg.rf.1ca0b649722d698060dfd60d565db381.jpg\n",
            "si_00002.jpg.rf.7d22c2070b6fd1bb6ece55ca4f7c6441.jpg\n",
            "si_00003.jpg.rf.16c4a5ebdec07b189a4c59c8e99b4dba.jpg\n",
            "si_00003.jpg.rf.77ad5e9055ab536d8e7f62ebdde41490.jpg\n",
            "si_00004.jpg.rf.1f436f4ead8a7db244aaff13d8a68c72.jpg\n",
            "si_00004.jpg.rf.2241aa123adba92a0a5b59091924ea23.jpg\n",
            "si_00005.jpg.rf.5e7a2b6b6843a0bfd69d54597a4fe184.jpg\n",
            "si_00005.jpg.rf.b0c5e561acf4ffd231a9158999fb27df.jpg\n",
            "si_00006.jpg.rf.5ffa0800d472076907c6675cce17169a.jpg\n",
            "si_00006.jpg.rf.61b093c9097c1bd714f71be4ba271c21.jpg\n",
            "si_00007.jpg.rf.59426fd493bfa4bd23d6fe1a7088cb22.jpg\n",
            "si_00007.jpg.rf.fc7a7410de6439f5e9fa88c583817f4a.jpg\n",
            "si_00008.jpg.rf.03df088411a5f7aea1479b3917457942.jpg\n",
            "si_00008.jpg.rf.7c47dbcb49b08eece7caf7649047707b.jpg\n",
            "si_00009.jpg.rf.2f5f6187af9759463f84e704f5b3f8db.jpg\n",
            "si_00009.jpg.rf.7e1875ef74b513a0d518c52844c2f31b.jpg\n",
            "si_00010.jpg.rf.18c054f1092c806c75661d83804ed3a8.jpg\n",
            "si_00010.jpg.rf.ec31fe65a884ffde1be8a6039ec67d30.jpg\n",
            "si_00011.jpg.rf.c3ce8d22c3dc6ea28c7ec64a269e6ac0.jpg\n",
            "si_00011.jpg.rf.f18aeca04c45e4bfc9076d54f2811084.jpg\n",
            "si_00012.jpg.rf.44ca5e4aa3760b7c9c50a646a80328ce.jpg\n",
            "si_00012.jpg.rf.d88ecafa030290416571187a1acc1b6b.jpg\n",
            "si_00013.jpg.rf.b00500e2b5b232a12b2ece00b9533878.jpg\n",
            "si_00013.jpg.rf.e0107c6dc0371cef9cdb874f41d9af46.jpg\n",
            "si_00014.jpg.rf.12ef6b840f65eb828bdfd8a61d9e5018.jpg\n",
            "si_00014.jpg.rf.900293ff121d2bde7b5b624aa925ceb7.jpg\n",
            "si_00015.jpg.rf.758d1ed5e5c061e956d7bae93d515576.jpg\n",
            "si_00015.jpg.rf.f79b0e5650157c061550b0e97a7e55ef.jpg\n",
            "si_00016.jpg.rf.b1dd950f00aaf980bb896941b5e30aee.jpg\n",
            "si_00016.jpg.rf.b4279f02a89be3b75ce1c891a1b6da74.jpg\n",
            "si_00017.jpg.rf.16904c7359d026f5f4a1ec8420fdd93c.jpg\n",
            "si_00017.jpg.rf.9c6a128a2f372e9ee72677cfd7ad31f2.jpg\n",
            "si_00018.jpg.rf.0bb4db7bd841d1b871cf6532a0b5586b.jpg\n",
            "si_00018.jpg.rf.c431abac60ae55bc4692aa06b8cf1f01.jpg\n",
            "si_00019.jpg.rf.9a39beb07496182c8a9a02409c752481.jpg\n",
            "si_00019.jpg.rf.d053a9bd017a06eeb4087681fbdce657.jpg\n",
            "si_00020.jpg.rf.38799897123a17bebfd8a3baa9a71bb1.jpg\n",
            "si_00020.jpg.rf.8b0b0e812e47f28fcd828342238e9efb.jpg\n",
            "si_00021.jpg.rf.679025d3238207bb0b0ee0a62eccad8d.jpg\n",
            "si_00021.jpg.rf.ae492dc89c1e3500f5cd38979417b510.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B93gz7K76noZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mv * ../"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v-5kQnZ6tg8",
        "colab_type": "code",
        "outputId": "9c5feec8-e76a-42a6-bfff-de9a576d7bed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/keras-yolo3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CW0XjFo6v3T",
        "colab_type": "code",
        "outputId": "0926d0ed-f4ca-437a-b153-5bf9c908a208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%ls"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_annotations.txt\n",
            "ch_00000.jpg.rf.3a408a1931e4a5721caada84d50ca3cb.jpg\n",
            "ch_00000.jpg.rf.86aede3bcf9d2e81bf05c91f453f024f.jpg\n",
            "ch_000010.jpg.rf.05e8c28282cc39a1f746ab3b02411990.jpg\n",
            "ch_000010.jpg.rf.7a067e8b9f1be408ea9d0732eaddd0aa.jpg\n",
            "ch_000011.jpg.rf.9856f4fc2275e5e06dcada930b26153f.jpg\n",
            "ch_000011.jpg.rf.a2f76cf2eca5af767c7a9a69ee1c98e2.jpg\n",
            "ch_000012.jpg.rf.d25a8aec1de84d98641462ff7bdf9e6d.jpg\n",
            "ch_000012.jpg.rf.fdb4da6ffa8606dcd751d6a26ef188e9.jpg\n",
            "ch_000013.jpg.rf.8af38c5c7d6d3d3385b9f59e72ccd6a1.jpg\n",
            "ch_000013.jpg.rf.faa4ba4c95b6f8446b3ccb94e3d83aa2.jpg\n",
            "ch_000014.jpg.rf.8d63bf81f69b73194151f6133ab26d20.jpg\n",
            "ch_000014.jpg.rf.ec6f40238a0978f2059aeb4de8f719e3.jpg\n",
            "ch_000015.jpg.rf.89e135a874bbcd5dfb73ba2f0f7c4eb4.jpg\n",
            "ch_000015.jpg.rf.f78ace876b15edfafa080a484d76cd9d.jpg\n",
            "ch_000016.jpg.rf.389923bee126feffd6b410b09aac0299.jpg\n",
            "ch_000016.jpg.rf.6b8e1c9a1b4ad0a67ba78bdef3f3ed6f.jpg\n",
            "ch_000017.jpg.rf.129d7ff1929cc1630165ccfa18715eaa.jpg\n",
            "ch_000017.jpg.rf.1b68f12cdbacc521c0df4c447f1606b7.jpg\n",
            "ch_000018.jpg.rf.a0cbb237d291dd9f75244dfab892857c.jpg\n",
            "ch_000018.jpg.rf.b20286a4c4d7e4f7849c4c439741ee1e.jpg\n",
            "ch_00001.jpg.rf.619340a0f891485382e0f97a060423a4.jpg\n",
            "ch_00001.jpg.rf.f22d7270f658f07bb23bcca977be3cdc.jpg\n",
            "ch_00002.jpg.rf.7169498bdf598df2f017be6432c20df4.jpg\n",
            "ch_00002.jpg.rf.ada2d2468fca43d0747e9bf7257b3b5b.jpg\n",
            "ch_00003.jpg.rf.1bcda9516e7fd0f9fefd2fa24afd000c.jpg\n",
            "ch_00003.jpg.rf.d618eb06a83cfbbfc982812326a552dc.jpg\n",
            "ch_00004.jpg.rf.3d8e2b97b86eb424e8d6dfea52e4b536.jpg\n",
            "ch_00004.jpg.rf.7dfd834640355c34a23bdd709fdac44d.jpg\n",
            "ch_00005.jpg.rf.356bee9aa07bd5ce14285ca198f4da94.jpg\n",
            "ch_00005.jpg.rf.c1383eb5ecc8e16026ab2f404dc0ed98.jpg\n",
            "ch_00006.jpg.rf.54e009367e7d4c95497c95b804b28fc3.jpg\n",
            "ch_00006.jpg.rf.617743ab1df4cbb68c5dd55254c7bdf3.jpg\n",
            "ch_00007.jpg.rf.066fc187e236517b2de6a24c3ed28f3b.jpg\n",
            "ch_00007.jpg.rf.90442f7d29ec708f6c1093f038829667.jpg\n",
            "ch_00008.jpg.rf.0e30446348fce76136ec0567bf3f28d5.jpg\n",
            "ch_00008.jpg.rf.f5f7cda23e8320b1224befcd0c91f3e5.jpg\n",
            "ch_00009.jpg.rf.2ae90e1c5b0de4436c4e0ae0ae3834e3.jpg\n",
            "ch_00009.jpg.rf.3676e9543ad5fd41fb27febcb32c46da.jpg\n",
            "ch_00019.jpg.rf.42fc004996aa563875f2d3a4756b37af.jpg\n",
            "ch_00019.jpg.rf.7e70d70127c55a3c7505c1a6db0fac7b.jpg\n",
            "ch_00020.jpg.rf.34a2e52bf8141639272c4cc8f777da5f.jpg\n",
            "ch_00020.jpg.rf.dc58edbab4edfa565f8b478f0ea83a55.jpg\n",
            "ch_00021.jpg.rf.78d41ae01b967dfa1115405c160ac3b1.jpg\n",
            "ch_00021.jpg.rf.82203afd3e75a6631c2e94a635797415.jpg\n",
            "_classes.txt\n",
            "coco_annotation.py\n",
            "convert.py\n",
            "darknet53.cfg\n",
            "de_00000.jpg.rf.3448473fa4a76b3e06b89f498b36477b.jpg\n",
            "de_00000.jpg.rf.f00db57bed78cdeb4805062e8584be94.jpg\n",
            "de_00001.jpg.rf.6b8fc96239144596a1bfaa52590c14fb.jpg\n",
            "de_00001.jpg.rf.9860ace2801f2902daf38d56c30ebb99.jpg\n",
            "de_00002.jpg.rf.49fe8e3e5179c7c2c1fa179cac555b04.jpg\n",
            "de_00002.jpg.rf.ce70f94e188ee7fbc9310a947d5d2d10.jpg\n",
            "de_00003.jpg.rf.9b0bb3c0ebcee7e5ecdc9defaea06a08.jpg\n",
            "de_00003.jpg.rf.afab645175e4812088dca1389dd35066.jpg\n",
            "de_00004.jpg.rf.31a284e8c2c0a7af0caa98a0fd2d5da4.jpg\n",
            "de_00004.jpg.rf.759b26e97d72e71d7832dc106d569bae.jpg\n",
            "de_00005.jpg.rf.303665fd06a407fcd21eae54f55c3dd4.jpg\n",
            "de_00005.jpg.rf.68e0b103b9db4ab9e91c5603a1dad26a.jpg\n",
            "de_00006.jpg.rf.5c4b7635f640772f59438891a72a6d33.jpg\n",
            "de_00006.jpg.rf.9ea4cd4ca650d67c45c9957c0f0ed5cf.jpg\n",
            "de_00007.jpg.rf.3cbf53013a2dbaebd5502b3edf5c4c16.jpg\n",
            "de_00007.jpg.rf.75200ab25ec597abf07b76d48dedb237.jpg\n",
            "de_00008.jpg.rf.a2e50661efdbc16273ae436e4c37d268.jpg\n",
            "de_00008.jpg.rf.b83061f1f9fcb81ebfc824929e15c162.jpg\n",
            "de_00009.jpg.rf.2f9e9d5f6b4d5197d1701997a5f4b10e.jpg\n",
            "de_00009.jpg.rf.bef9fb521b64136e9147671b42a8b35c.jpg\n",
            "de_00010.jpg.rf.08fe7088c5175cecbba205c31f8ac38e.jpg\n",
            "de_00010.jpg.rf.7917a8d74c2497a6f081e3c5fd41a056.jpg\n",
            "de_00011.jpg.rf.73d69d14308dc890ee252aa0a7ca17ce.jpg\n",
            "de_00011.jpg.rf.f3b9759f4388caa7bce574c482e74b0a.jpg\n",
            "de_00012.jpg.rf.32a823a6ee50446d7f3285dbbe88fcd1.jpg\n",
            "de_00012.jpg.rf.eb11b398a9cd8796ed5ee93c83098979.jpg\n",
            "de_00013.jpg.rf.81096463dcdb882f0060e8fc2490e7ca.jpg\n",
            "de_00013.jpg.rf.b20c5163ed5b35e4c96dca3ab849e628.jpg\n",
            "de_00014.jpg.rf.051c039d80271b1751b87aff33edceed.jpg\n",
            "de_00014.jpg.rf.87a12f55a50b4b3636ac0874e5384411.jpg\n",
            "de_00015.jpg.rf.31caabf1141f31f20c643f2c8f3db517.jpg\n",
            "de_00015.jpg.rf.9fc7949bb93d30840d0e65d48cf8c143.jpg\n",
            "de_00016.jpg.rf.8eff6151adc63c3a4bd23b1908ef912c.jpg\n",
            "de_00016.jpg.rf.cc9f86e643b728f64aa401c68320b8ad.jpg\n",
            "de_00017.jpg.rf.b15546bf29ed3e0fb5b692f446842c1f.jpg\n",
            "de_00017.jpg.rf.fa958d7d54aab37ce5992de5017a1953.jpg\n",
            "de_00018.jpg.rf.0edcad9fae8d601395bac9f6af94d10c.jpg\n",
            "de_00018.jpg.rf.f4f0f59fedd0f5d3d22dbfafb73e144b.jpg\n",
            "es_00000.jpg.rf.7ad53ac639651904947b9774f6190291.jpg\n",
            "es_00000.jpg.rf.8ecd4beeb207f24ebe3c71ef4e208a42.jpg\n",
            "es_00001.jpg.rf.b6d50ae67796c53e355ef389b048468d.jpg\n",
            "es_00001.jpg.rf.d163405daa41ae749437990b58d0df39.jpg\n",
            "es_00002.jpg.rf.798e9ef20fbb246406755677329488f4.jpg\n",
            "es_00002.jpg.rf.e2349be517edb678f20606b4d23fc726.jpg\n",
            "es_00003.jpg.rf.233df977c7c83ddffc851e478d5f3e3f.jpg\n",
            "es_00003.jpg.rf.5612006372b0c2bb3f85c0191f372867.jpg\n",
            "es_00004.jpg.rf.1f7d2afd01479da137e19d17d287bb97.jpg\n",
            "es_00004.jpg.rf.750873f276df3b6a4aa16ca0f49baa29.jpg\n",
            "es_00005.jpg.rf.0fad5024243923b073bedf4756908cd9.jpg\n",
            "es_00005.jpg.rf.8b634d843290f6eaf74dda6af2f3b6a1.jpg\n",
            "es_00006.jpg.rf.01762b3ea80d3d1ea50a18a8398bb943.jpg\n",
            "es_00006.jpg.rf.bcfb088064b1a70d4073fe508b07e0d6.jpg\n",
            "es_00007.jpg.rf.04506fe9483c61a2734ef6a87ecf8941.jpg\n",
            "es_00007.jpg.rf.cb1a65e14657429b99efca6063009566.jpg\n",
            "es_00008.jpg.rf.a908cd878bb0fd597de7a21e7e91b3ec.jpg\n",
            "es_00008.jpg.rf.ba97e93e4983425a60ef7b206197d375.jpg\n",
            "es_00009.jpg.rf.282d23d90f738b534fce016b49ce8f56.jpg\n",
            "es_00009.jpg.rf.5a1fd78945f740cf4a1ae8db4f8cae46.jpg\n",
            "es_00010.jpg.rf.e34e5db54c7c8ebf18f0a27cd0a0a7fd.jpg\n",
            "es_00010.jpg.rf.ee04e7e83d8e8528875a4c194bf33897.jpg\n",
            "es_00011.jpg.rf.08cadef83c16bed6174f7d9c1cd0acf0.jpg\n",
            "es_00011.jpg.rf.da72fe28192c54050a9f3afd0f4b6f13.jpg\n",
            "es_00012.jpg.rf.ed9317c60a0646818e7a7c1a4906838a.jpg\n",
            "es_00012.jpg.rf.f2127a7f62496343d048923f90d2ce03.jpg\n",
            "es_00013.jpg.rf.6723627b9c88373235439f76f703c997.jpg\n",
            "es_00013.jpg.rf.a593cd36bbfadf19a4f65df8076c3cbf.jpg\n",
            "es_00014.jpg.rf.2fc917dc01f2f893d5b53ca9052978a1.jpg\n",
            "es_00014.jpg.rf.649d616f8d2958826b22423279c72780.jpg\n",
            "es_00015.jpg.rf.4e8ec3eafb0d81eb863779821b340b7d.jpg\n",
            "es_00015.jpg.rf.dfccd3cd0496a41f6ed2512514482ec5.jpg\n",
            "es_00016.jpg.rf.195c007d148c0ecbe1272ac327975549.jpg\n",
            "es_00016.jpg.rf.bd1afd7019e28fb0b0e9fa7974c9d812.jpg\n",
            "es_00017.jpg.rf.6802ba3a42c61f9681409656a35cc310.jpg\n",
            "es_00017.jpg.rf.98cd25063e8baa7fe3b1fd552c45fb18.jpg\n",
            "es_00018.jpg.rf.8c86228b657f8580ddd2df2fb95dea21.jpg\n",
            "es_00018.jpg.rf.b7d0d425ef24d290e74d75eebc2a9504.jpg\n",
            "es_00019.jpg.rf.62c72012aa22dad784c6abc058939de0.jpg\n",
            "es_00019.jpg.rf.e57ed7df62262f405e77a1a98938f955.jpg\n",
            "es_00020.jpg.rf.31e9eb8c9e94a966dcc48cf4d5e20304.jpg\n",
            "es_00020.jpg.rf.54eedb612eb6b00ee6b0add9b52a7fae.jpg\n",
            "es_00021.jpg.rf.64a52077e6df3f1514f24d4f8f043d5c.jpg\n",
            "es_00021.jpg.rf.702ad0e2882c61b4db371c052bd4da9a.jpg\n",
            "es_00022.jpg.rf.e426120a5e88fd31bbe2dbcdb2529c81.jpg\n",
            "es_00022.jpg.rf.ec84510277f0b44c4931bb526a2a6b5d.jpg\n",
            "es_00023.jpg.rf.11fcc5cbfdce683e96694dc2e1a94f64.jpg\n",
            "es_00023.jpg.rf.557ff36d82cc163a42a781f2c7d34942.jpg\n",
            "es_00024.jpg.rf.038453c3c3fe6c4dd05ef9c51f662e32.jpg\n",
            "es_00024.jpg.rf.9729ac86c102d55bda5e6f8ccb603ad0.jpg\n",
            "es_00025.jpg.rf.0652fe2a16f336088f40f542b7e12521.jpg\n",
            "es_00025.jpg.rf.ac57a12aedaee36e62c5286ca705cb7b.jpg\n",
            "\u001b[0m\u001b[01;34mexport\u001b[0m/\n",
            "\u001b[01;34mfont\u001b[0m/\n",
            "fr_00000.jpg.rf.a72aa9d0477b9436fc0b47b16dba5894.jpg\n",
            "fr_00000.jpg.rf.c7ef6714a672b53e79f839c65209baf0.jpg\n",
            "fr_00001.jpg.rf.09400b0497c4261ee6b7789ccd16a180.jpg\n",
            "fr_00001.jpg.rf.c455b718bea7a640f586a91bde2e83ba.jpg\n",
            "fr_00002.jpg.rf.32c47d0b1fe01971724b1ca6448350a8.jpg\n",
            "fr_00002.jpg.rf.62b8a1fede967c495c8ed938ad9b4f67.jpg\n",
            "fr_00003.jpg.rf.2b7f7200cdb0043a4ee35a60da05ff0c.jpg\n",
            "fr_00003.jpg.rf.d4ff49cd9f8805d497983ccb7936465f.jpg\n",
            "fr_00004.jpg.rf.00cf20797473c6ef6d881c3c11ce7f70.jpg\n",
            "fr_00004.jpg.rf.44f7b61c2495fc4b7e0f650db6bdb996.jpg\n",
            "fr_00005.jpg.rf.bfa531d6eaa242f5784fa74818ff63b4.jpg\n",
            "fr_00005.jpg.rf.c55a71457a1361ea024dffa25ea3731b.jpg\n",
            "fr_00006.jpg.rf.9d8de1e1aa45ae8e3f4ed94cf7b02521.jpg\n",
            "fr_00006.jpg.rf.ffd58b316357a3d0957b8116046c2463.jpg\n",
            "fr_00007.jpg.rf.50ec4d6b8d7b5c4091af82edbd1f5710.jpg\n",
            "fr_00007.jpg.rf.d6d7de0a196f63f2244a84d732a2c2af.jpg\n",
            "fr_00008.jpg.rf.5a371281c7489acf7e12d1f0fd309523.jpg\n",
            "fr_00008.jpg.rf.c02bbb808a2e7f51de3701335c9b31cc.jpg\n",
            "fr_00009.jpg.rf.c2c13bbc8e5f26a5b659e0f7815c7047.jpg\n",
            "fr_00009.jpg.rf.f51425f2bf3d18070b572daee2004dd8.jpg\n",
            "fr_00010.jpg.rf.069bb973a6f2a8ba7be2ce196de820e6.jpg\n",
            "fr_00010.jpg.rf.fb3b20a611c7f527697e817e1f53f3d1.jpg\n",
            "fr_00011.jpg.rf.1e5cb872cb6cd59e3fdbbb117841ed74.jpg\n",
            "fr_00011.jpg.rf.d763a9071cdbbf10d1349086f0e0427b.jpg\n",
            "fr_00012.jpg.rf.4ee55e8ff4e8c6d9dbd79075fbc1b10b.jpg\n",
            "fr_00012.jpg.rf.f79c53de5040566632f2ac1747b5a28b.jpg\n",
            "fr_00013.jpg.rf.8127f7b63bc6c719baec95ad23f0628f.jpg\n",
            "fr_00013.jpg.rf.f698f4c666aecb7ec9ac3138065d1dee.jpg\n",
            "gr_00000.jpg.rf.d5cb01cf64bbff61108a800622d77248.jpg\n",
            "gr_00000.jpg.rf.ee8e57b2f81b2f466c3bbcf094327853.jpg\n",
            "gr_00001.jpg.rf.a4c52f9a1841192d071d97d301dffc5e.jpg\n",
            "gr_00001.jpg.rf.f5e8cd4ee91b112064adf93d6ca59e7a.jpg\n",
            "gr_00002.jpg.rf.150f624a29d296dca6faaa64e93143a3.jpg\n",
            "gr_00002.jpg.rf.d9ab6d4b87c2cf54d93f6474406bc83a.jpg\n",
            "gr_00003.jpg.rf.7b9e148bc76967d180ece5a2f70b4cc7.jpg\n",
            "gr_00003.jpg.rf.9abe6962bab00b2345665517a11a8b5e.jpg\n",
            "gr_00004.jpg.rf.438a9801e81d43b499cd5dc421c13b27.jpg\n",
            "gr_00004.jpg.rf.fb824e2d95952bc369e43150288fe60e.jpg\n",
            "gr_00005.jpg.rf.66d69c3ba94e16bec1b4d9371e78cb99.jpg\n",
            "gr_00005.jpg.rf.900a5768b046c3dac32367547ef89e46.jpg\n",
            "gr_00006.jpg.rf.8405927c6eae856fc37e68b325ab2233.jpg\n",
            "gr_00006.jpg.rf.f946131fc4a245d31386f3096a48014f.jpg\n",
            "gr_00007.jpg.rf.0d9107e4cd565a1cc484679e507f6505.jpg\n",
            "gr_00007.jpg.rf.ebde6ca2adad7afc51968eb82e8d2299.jpg\n",
            "gr_00008.jpg.rf.556d1e6dc057f6d758608b0ebeeeb480.jpg\n",
            "gr_00008.jpg.rf.9f06f46201038468c2617e85140fc8de.jpg\n",
            "gr_00009.jpg.rf.c4cf25e2da336bf50a8a288200800d04.jpg\n",
            "gr_00009.jpg.rf.f6f8a743b1e366242ad98f66baef84a5.jpg\n",
            "gr_00010.jpg.rf.0125e1d9d2f16e8c8cc5aeecc2d64ca4.jpg\n",
            "gr_00010.jpg.rf.c263a7c644f5746473339019d1d49f4a.jpg\n",
            "gr_00011.jpg.rf.34087e3d2ac4e00c83b75e0801d781d7.jpg\n",
            "gr_00011.jpg.rf.74d609d1e07d9376e9aafef1061b9a86.jpg\n",
            "gr_00012.jpg.rf.3e6a7c3bc78e6dca286249a11f299378.jpg\n",
            "gr_00012.jpg.rf.7d27b2a3a6ce00aea67fe35b0477eb59.jpg\n",
            "gr_00013.jpg.rf.9bb859cea19f23a798d5ab78f92bd75f.jpg\n",
            "gr_00013.jpg.rf.cb2fd2b1ae38014e455e37478992cb56.jpg\n",
            "gr_00014.jpg.rf.158600343ed0a8441b55cef2305d0ec9.jpg\n",
            "gr_00014.jpg.rf.554e5662fbe95c4bc0bec7f588d764fe.jpg\n",
            "gr_00015.jpg.rf.8a0cdeb3e9a6a88c0248c5d67a101f9f.jpg\n",
            "gr_00015.jpg.rf.c67e6b89c9044963a12ad21355595140.jpg\n",
            "gr_00016.jpg.rf.b293451355d0d0e6c740b63a69023c7d.jpg\n",
            "gr_00016.jpg.rf.daa2a843a0ca99f76fe2b503c5dffd79.jpg\n",
            "gr_00017.jpg.rf.728a4f4fbddcedea9b82925b95595929.jpg\n",
            "gr_00017.jpg.rf.92575aba13a94575ee54c40be34f616a.jpg\n",
            "gr_00018.jpg.rf.08f9b5a1e30c4dbdfdb5ef86c2e3f23e.jpg\n",
            "gr_00018.jpg.rf.971353df061e65dc2852f0ff27b9a793.jpg\n",
            "gr_00019.jpg.rf.cf10c12f7f306ed5f6f875a9855e4012.jpg\n",
            "gr_00019.jpg.rf.daf16e8dea0aef67a1f72f29aa33b50b.jpg\n",
            "hu_00000.jpg.rf.3e63f649b84e331b332fe49398a54d03.jpg\n",
            "hu_00000.jpg.rf.da408b5614a5a07dfdbbf7afee10d4f3.jpg\n",
            "hu_00001.jpg.rf.728bcc706d706b732fd9eb8b20214be9.jpg\n",
            "hu_00001.jpg.rf.ccd4a23aa5d1bd7b7f827bad82b0e740.jpg\n",
            "hu_00002.jpg.rf.320ed34b24f25b0314683e55f599eecc.jpg\n",
            "hu_00002.jpg.rf.ccfafc2e3d6573bd4d5f9353395cbacf.jpg\n",
            "hu_00003.jpg.rf.428ae76d00c5545a351245ecbdb9cea3.jpg\n",
            "hu_00003.jpg.rf.9cf4a737b10d694de421c99dd967f165.jpg\n",
            "hu_00004.jpg.rf.1cf47a5ff2acd9fce4ed89e2444fc000.jpg\n",
            "hu_00004.jpg.rf.54951122dc9dbae564445072ef608fc5.jpg\n",
            "hu_00005.jpg.rf.4e9ccbcdd8a222751777dd9319106c6a.jpg\n",
            "hu_00005.jpg.rf.f3709ed59787f1d1cefa5b1a619d8552.jpg\n",
            "hu_00006.jpg.rf.bbbaa0274e39211bbdc844d87a401a0d.jpg\n",
            "hu_00006.jpg.rf.cedb720729c2a2478a452cdc3e0f49f2.jpg\n",
            "hu_00007.jpg.rf.51181f8c43320c7977cd7a91342c5c4e.jpg\n",
            "hu_00007.jpg.rf.6ca20d896c92eb63832053926067c4e8.jpg\n",
            "hu_00008.jpg.rf.5bf39d6145ce9242f96e8251a35a4afa.jpg\n",
            "hu_00008.jpg.rf.f48c9340dfe0865836ebf0c60666e615.jpg\n",
            "hu_00010.jpg.rf.4528519852d92d2b342e191526416248.jpg\n",
            "hu_00010.jpg.rf.607687716f75880c7a53387cea663186.jpg\n",
            "hu_00011.jpg.rf.79d392120c1675fc91c768d1d844f110.jpg\n",
            "hu_00011.jpg.rf.ea0515d0a35d3d073838c1c6103c011a.jpg\n",
            "hu_00012.jpg.rf.3c0d71211c89c7f138a832fba33ede47.jpg\n",
            "hu_00012.jpg.rf.aed997b2f16261c5700dff0c5c8a39e3.jpg\n",
            "hu_00013.jpg.rf.76d70c9267cbfd194e4b2abfb739e209.jpg\n",
            "hu_00013.jpg.rf.a5acd2a25811fcc7723547b9d6d276e5.jpg\n",
            "hu_00015.jpg.rf.2024aeeea0d84dca750d4adc8c8c637d.jpg\n",
            "hu_00015.jpg.rf.c1af265fb277c32d311f330f5be257ef.jpg\n",
            "hu_00016.jpg.rf.6166f76d910d47a471684d102388105f.jpg\n",
            "hu_00016.jpg.rf.96b12c92b0eaa70e5e81d40687a6eb7b.jpg\n",
            "kmeans.py\n",
            "LICENSE\n",
            "me_00000.jpg.rf.5b49d8363b1d722f9e851114b6af1294.jpg\n",
            "me_00000.jpg.rf.bd710d2668f56fd162f862dbfdafad6e.jpg\n",
            "me_00001.jpg.rf.079cde12e7b6d606e47c70a1e16d7c8f.jpg\n",
            "me_00001.jpg.rf.3bd7fdb4e928616f4fc9af4d6aa01fa1.jpg\n",
            "me_00002.jpg.rf.9cd48244747570c0e7ba3c5b82d15e99.jpg\n",
            "me_00002.jpg.rf.c1791511d9911ed099c418da70729ddb.jpg\n",
            "me_00003.jpg.rf.591c9c2a47f6884475184ac28f7909e1.jpg\n",
            "me_00003.jpg.rf.f8bc6a45e30bf79024fb3e518f636ed0.jpg\n",
            "me_00004.jpg.rf.669b5b71d145db7029a4ad4c75a96f4f.jpg\n",
            "me_00004.jpg.rf.9bed0e01fb221402d59ee3872a859faf.jpg\n",
            "me_00005.jpg.rf.6d78d33185e036d931a532912a60fa8c.jpg\n",
            "me_00005.jpg.rf.fb25d1cae228a5f8b9082eb293ac66d9.jpg\n",
            "me_00006.jpg.rf.384a63ad66a57ae5be1ccf8f593de222.jpg\n",
            "me_00006.jpg.rf.fc47a89da6dacd2a5e1e1c37420655fc.jpg\n",
            "me_00007.jpg.rf.053395b7ceb0fa5a43988ace2bd100ae.jpg\n",
            "me_00007.jpg.rf.bc3a925da8a2756951863ca3f2a47e59.jpg\n",
            "me_00008.jpg.rf.38b416930355ae0a199ec6372b8b4bae.jpg\n",
            "me_00008.jpg.rf.a64bf8a848354843288569c6dee8a783.jpg\n",
            "me_00009.jpg.rf.c11cf7c4933a25c958ab436a3d436088.jpg\n",
            "me_00009.jpg.rf.cad6cc8a72629cee90cafb078de3f2b9.jpg\n",
            "me_00010.jpg.rf.23500ba05c3649629e02185a436c65e5.jpg\n",
            "me_00010.jpg.rf.a349f17ad152f5fc5a47d3b8a2439196.jpg\n",
            "me_00011.jpg.rf.7f69818805e1f13f1ecb2a08f38e099c.jpg\n",
            "me_00011.jpg.rf.a0113e48820b5b92dee9a5c620dada71.jpg\n",
            "me_00012.jpg.rf.8fe99cd9e71b3ef858ac2ef40a96c2de.jpg\n",
            "me_00012.jpg.rf.9422e0f868bbc1a540ca2532585e0aad.jpg\n",
            "me_00013.jpg.rf.1643c06bb0984006c53ac01266b74fd5.jpg\n",
            "me_00013.jpg.rf.d3a655453c8ae545a8bcf9c5f700ad2a.jpg\n",
            "me_00014.jpg.rf.37337144b9d6e778882e30685c1ab8f9.jpg\n",
            "me_00014.jpg.rf.ceecce07025da89383ce4b303e42282b.jpg\n",
            "me_00015.jpg.rf.18d2f2fa3aaf06cf8e85f6fc34b45c46.jpg\n",
            "me_00015.jpg.rf.db391f1919e8b9802653ba829f342e04.jpg\n",
            "\u001b[01;34mmodel_data\u001b[0m/\n",
            "pt_00000.jpg.rf.1b3167a38dce0a33e998efbaaf6de28a.jpg\n",
            "pt_00000.jpg.rf.e54bb0a9d1b8bfd3f981e10470f33d2b.jpg\n",
            "pt_00001.jpg.rf.966691a7319c957dd63e6939363c88d4.jpg\n",
            "pt_00001.jpg.rf.a799954727943d3ba4131fd627154030.jpg\n",
            "pt_00002.jpg.rf.5eb3493094c9a98ff2503ccb89a96bac.jpg\n",
            "pt_00002.jpg.rf.b633af26a1b33a8276632faeab1176ce.jpg\n",
            "pt_00003.jpg.rf.b476adbad7cb2e914da9baa5007d5884.jpg\n",
            "pt_00003.jpg.rf.d9113545355a40f7ea02bd4d02063ecd.jpg\n",
            "pt_00004.jpg.rf.19c7c758028fdacda7154b6634626e3e.jpg\n",
            "pt_00004.jpg.rf.6109dfaf483e642be978a1f50db40e23.jpg\n",
            "pt_00005.jpg.rf.1aa142b8179dccb5991d97a4ca8d79d5.jpg\n",
            "pt_00005.jpg.rf.9e639382058fc4cd83020bd3e6441bcd.jpg\n",
            "pt_00006.jpg.rf.12cb3f1a01b2a6c36894735c25848c47.jpg\n",
            "pt_00006.jpg.rf.e56788fa3acfa1244b9ccc1b26769e5a.jpg\n",
            "pt_00007.jpg.rf.93afa937df4ce8bd145eb661ea144323.jpg\n",
            "pt_00007.jpg.rf.fd7659e178e9166ce33ee2c30c4bc81e.jpg\n",
            "pt_00008.jpg.rf.099101bb0e3111075612617d07f19fa8.jpg\n",
            "pt_00008.jpg.rf.1f6466f356f2b88b9f9c63761d0d7bcb.jpg\n",
            "pt_00009.jpg.rf.30a082eff0e20961593fe54f92fdad21.jpg\n",
            "pt_00009.jpg.rf.82cbc13c70e3abb77a95b5567fc026a3.jpg\n",
            "pt_00010.jpg.rf.87d89229db6ef8f6201faf18a500e023.jpg\n",
            "pt_00010.jpg.rf.95b34a89122f884e8e069c61a3a707e6.jpg\n",
            "pt_00011.jpg.rf.34a39d46b3be0500645c7047fd470770.jpg\n",
            "pt_00011.jpg.rf.76c1353837b187cf4cf85f006f2e09a4.jpg\n",
            "pt_00012.jpg.rf.33d1e02c740e181662a542ad05402040.jpg\n",
            "pt_00012.jpg.rf.bf2ed226095d71063c2af1e3e824c63b.jpg\n",
            "pt_00013.jpg.rf.06f99ace0c399b4145e9e27f2615d1c5.jpg\n",
            "pt_00013.jpg.rf.bafaee71a5307e9a1c0bccd940859517.jpg\n",
            "pt_00014.jpg.rf.628ed5ad0d2b78c8eccc52d9c9de8358.jpg\n",
            "pt_00014.jpg.rf.f11307dc1a3b35619d2badffb7932e90.jpg\n",
            "pt_00015.jpg.rf.2835271fdce493dafaddf9d0c2cbae2f.jpg\n",
            "pt_00015.jpg.rf.4525ce0abd32d19c2e3bc321f4b3dd30.jpg\n",
            "pt_00016.jpg.rf.78b9a4c0468c7ae6738e17f3f1142522.jpg\n",
            "pt_00016.jpg.rf.f9354e6e3daca2ad33fc50efb4b6f37f.jpg\n",
            "pt_00017.jpg.rf.630478ab2e29cd2d0cabf323e90f9078.jpg\n",
            "pt_00017.jpg.rf.d0f3f8863514bd4a7d1b44370d152d38.jpg\n",
            "pt_00018.jpg.rf.07c198490d2d6750d92931a17294f4bb.jpg\n",
            "pt_00018.jpg.rf.7a48b50f2dbc2ec9d7c5141bd370742b.jpg\n",
            "pt_00019.jpg.rf.9ddd1613bdecfae16cd7cb38bed6711e.jpg\n",
            "pt_00019.jpg.rf.bb1d427c38a9a523695cd9706a5a4b3f.jpg\n",
            "README.md\n",
            "README.roboflow.txt\n",
            "rs_00000.jpg.rf.5d131dff8a80dab17a9fb81f4dc23fc0.jpg\n",
            "rs_00000.jpg.rf.a7f7e7dd2e22d5fca1e17516e1ff005b.jpg\n",
            "rs_00001.jpg.rf.3b434e92c8ca865bd55d65c335a2c5ed.jpg\n",
            "rs_00001.jpg.rf.618659509637b8acbeb65b33dd5a56cf.jpg\n",
            "rs_00002.jpg.rf.0a0342e57b8f44f5e1e7067d4c46722a.jpg\n",
            "rs_00002.jpg.rf.8eb2c15620b938ecb8ca978f74018c02.jpg\n",
            "rs_00003.jpg.rf.351e94dd996c00fd101ad3c62cdca4c3.jpg\n",
            "rs_00003.jpg.rf.9c136f72dd4bcae5109d8b22a672d076.jpg\n",
            "rs_00004.jpg.rf.18ff27f5758488112ff10a0cad6b9986.jpg\n",
            "rs_00004.jpg.rf.8695c573e0dbde9f9813fa7be47adda0.jpg\n",
            "rs_00005.jpg.rf.dd7944a02dc4720bb6f568199aa23d11.jpg\n",
            "rs_00005.jpg.rf.f7ca8176ec3fb4b2461b6360d2727dda.jpg\n",
            "rs_00006.jpg.rf.10254448501351d957b20786b5256b25.jpg\n",
            "rs_00006.jpg.rf.7518dc78ae1f2c961b12f49cda290967.jpg\n",
            "rs_00007.jpg.rf.4890b66bc58247580144777bc83da2ac.jpg\n",
            "rs_00007.jpg.rf.9e892c71330fd0598188690f145e06f5.jpg\n",
            "rs_00008.jpg.rf.99ed6634d3b6a9f4f32702bd5b6a7e45.jpg\n",
            "rs_00008.jpg.rf.cf075d67b4500fcad736b35f55bc3667.jpg\n",
            "rs_00009.jpg.rf.08e52e059e6c12312c8732b57fea2996.jpg\n",
            "rs_00009.jpg.rf.a0d479b1a46c1c46ab8dedfff82600ae.jpg\n",
            "rs_00010.jpg.rf.609ecf806c32842ac984e500eda72f9b.jpg\n",
            "rs_00010.jpg.rf.d42bec3672cb57db57174c266dd45f20.jpg\n",
            "rs_00011.jpg.rf.3493fecbde2c8f8aba80bea49f6d3c46.jpg\n",
            "rs_00011.jpg.rf.d1149470d49e9e2ace439d29c1d5c360.jpg\n",
            "rs_00012.jpg.rf.2934e1b08ced2909d7ed129ef0419a3f.jpg\n",
            "rs_00012.jpg.rf.b595ae9fd868fadcf81384ced0c4a55a.jpg\n",
            "rs_00013.jpg.rf.38a8153439cb30151e85a934d83c6d95.jpg\n",
            "rs_00013.jpg.rf.3d332d83d933f70422694e2f70817767.jpg\n",
            "rs_00014.jpg.rf.49da48002e313e7d0ecf1644b65e6a03.jpg\n",
            "rs_00014.jpg.rf.d7d720a99a6fb9faf271d8e829cbe582.jpg\n",
            "rs_00015.jpg.rf.6670edfd704ea291bd7e5da1a7337511.jpg\n",
            "rs_00015.jpg.rf.7894990bdd474c0b4916bb5602e155df.jpg\n",
            "rs_00016.jpg.rf.4c1c2defdea93b0030ae112234056169.jpg\n",
            "rs_00016.jpg.rf.9b6d4656480f8bc12e4c17c08539d82b.jpg\n",
            "rs_00017.jpg.rf.10b4222e87aa05aa0c57830a98a9d9ef.jpg\n",
            "rs_00017.jpg.rf.91dc95b9eb50945d537eae3fc28099f7.jpg\n",
            "rs_00018.jpg.rf.a705f8300dbf49ff429f8496346fa1d1.jpg\n",
            "rs_00018.jpg.rf.f0fac0ebefb71d8a0e41c921773d6c79.jpg\n",
            "rs_00019.jpg.rf.45c34a8684e85f379c62e2ef43ebb0a7.jpg\n",
            "rs_00019.jpg.rf.9eb76c0b8fd7b558734cb9e91d6c25bf.jpg\n",
            "rs_00020.jpg.rf.efa54fe2a83ed7920810ef959625a979.jpg\n",
            "rs_00020.jpg.rf.fb3c1b8c3e869eb81f1e33482d84cc00.jpg\n",
            "rs_00021.jpg.rf.ad28fcdd4a8882a25d1e483ac326ef75.jpg\n",
            "rs_00021.jpg.rf.e507fdcc67cc26074bc4c8892097dceb.jpg\n",
            "rs_00022.jpg.rf.18a96de4400029e341ecdb2f8bcf7722.jpg\n",
            "rs_00022.jpg.rf.e75e55d6303f92d6a0ee9fa6657eea3e.jpg\n",
            "rs_00023.jpg.rf.5527b5625bd4f6e78fef95f7a51acece.jpg\n",
            "rs_00023.jpg.rf.82877aba78c30838020aa42063d3f746.jpg\n",
            "rs_00024.jpg.rf.07917acbb8b031cddf70f237bc6961f1.jpg\n",
            "rs_00024.jpg.rf.e2a0ab853bdb0dcaf036697953b64b60.jpg\n",
            "rs_00025.jpg.rf.0ebd22318326599314c0778ac6d6c43a.jpg\n",
            "rs_00025.jpg.rf.54b1e1a642ffb1fe4e2c03080b92ffd7.jpg\n",
            "rs_00026.jpg.rf.08e0b63805f6cc147662b859e4797ac0.jpg\n",
            "rs_00026.jpg.rf.4169305d3425bf1c82e386fd75ea4054.jpg\n",
            "rs_00027.jpg.rf.50b4bad6d992f0a91e1e1a84e158f0a6.jpg\n",
            "rs_00027.jpg.rf.9bc17496983795fec781f0af6c92f2c7.jpg\n",
            "rs_00028.jpg.rf.c16b1ec4ca75fc8d796dcd928915aa6c.jpg\n",
            "rs_00028.jpg.rf.fbbf0c591718f85e9f9ec2b7433ad981.jpg\n",
            "rs_00029.jpg.rf.55b9f55a19a5680f04671522ac12486d.jpg\n",
            "rs_00029.jpg.rf.6cc3b03861410ac841400a8267eece0a.jpg\n",
            "rs_00030.jpg.rf.67349b7bb3f6402105676a751a084388.jpg\n",
            "rs_00030.jpg.rf.8ebfb9022100e4160ebd62eac95fa378.jpg\n",
            "rs_00031.jpg.rf.c1b97f4391c617f7ec798f9f68743791.jpg\n",
            "rs_00031.jpg.rf.e2135613e46ca8d9b7bd675bb9db4644.jpg\n",
            "rs_00032.jpg.rf.91e43a154dcb4bd62fb985d765504f85.jpg\n",
            "rs_00032.jpg.rf.f1138cdabc7d2d357cabd4f1c0e15dd4.jpg\n",
            "rs_00033.jpg.rf.71a1da23b2290ebc8f6377fcca28979a.jpg\n",
            "rs_00033.jpg.rf.8bb373f0f8ebd716add7d0ed81f91ee8.jpg\n",
            "rs_00034.jpg.rf.5816075f55a1a3b45bbca2bfd0e9b6ad.jpg\n",
            "rs_00034.jpg.rf.bd7af706b1ce144f13698e819adf8e3c.jpg\n",
            "rs_00035.jpg.rf.64d95bdcc3d7ec76441a3fcd3ffc0e92.jpg\n",
            "rs_00035.jpg.rf.a480c46ea22ea9218fcbb2040ce353ed.jpg\n",
            "rs_00036.jpg.rf.020c0c36d1d231608e4a102ea7b290b6.jpg\n",
            "rs_00036.jpg.rf.4e2178b58055d5693d941abc4fa122e8.jpg\n",
            "rs_00037.jpg.rf.51f5b639736273833cdb2c0bb67ca8b7.jpg\n",
            "rs_00037.jpg.rf.c95c56edf449ac560554550e13836302.jpg\n",
            "rs_00038.jpg.rf.1953f4f30753091382d0c1fe0d87de52.jpg\n",
            "rs_00038.jpg.rf.3379b8282f5c3aba7707c07fc27f014a.jpg\n",
            "rs_00039.jpg.rf.3595fdb3e71b484d98b1f9295630990b.jpg\n",
            "rs_00039.jpg.rf.58f1ef9e46bbcb94d50b0fb236a7705f.jpg\n",
            "rs_00040.jpg.rf.5f5abd26e07a5b4600ce58770b80ddf1.jpg\n",
            "rs_00040.jpg.rf.fcdb0e566a53962f9319f755bd8e510e.jpg\n",
            "rs_00041.jpg.rf.04c43db05fdc7d4f9ad80c052048db3b.jpg\n",
            "rs_00041.jpg.rf.e28039df733822ed25af85c71914451b.jpg\n",
            "rs_00042.jpg.rf.5e08096a8228d75bd77e7d36a5a3162d.jpg\n",
            "rs_00042.jpg.rf.c069fc04666fa6208b5f09e31c65af55.jpg\n",
            "rs_00043.jpg.rf.3ba9996626ffd1ec29e6d324ecc5d5aa.jpg\n",
            "rs_00043.jpg.rf.f5295c61560178ec3bb5dc22fbaacc6b.jpg\n",
            "rs_00044.jpg.rf.2235f631beca436a26c1ef7e6c0e3007.jpg\n",
            "rs_00044.jpg.rf.eb62dc1c0ae5731184fe3820de4ff1ce.jpg\n",
            "rs_00045.jpg.rf.bbae68fe7d324d0c27c4efa5ecf29fc3.jpg\n",
            "rs_00045.jpg.rf.d6dd0e30aa332d92989fbed147779962.jpg\n",
            "rs_00046.jpg.rf.7ee5ac9fc84982312ff1cc8457faab9d.jpg\n",
            "rs_00046.jpg.rf.b3961b6673fe1f2fd5f8927d88ebd247.jpg\n",
            "rs_00047.jpg.rf.1417fd442f94006bdc2a8453a6837130.jpg\n",
            "rs_00047.jpg.rf.63653908d74000854034185993f2fc71.jpg\n",
            "rs_00048.jpg.rf.5be8a1106be57313d9282c3b94ec8259.jpg\n",
            "rs_00048.jpg.rf.7225ee0fc2b4d9728b485171ed615d73.jpg\n",
            "rs_00049.jpg.rf.24ba75a1ffd694d7c886a858e53e2615.jpg\n",
            "rs_00049.jpg.rf.3053143ccfa6205fa8c5678e9641e05d.jpg\n",
            "rs_00050.JPG.rf.10b42b589bbae7a399612a46daa360ca.jpg\n",
            "rs_00050.JPG.rf.47698ccfdca09a33d9fdc5f70924680d.jpg\n",
            "rs_00051.jpg.rf.5b8ebcf863e248eb167873cd5ce61d5a.jpg\n",
            "rs_00051.jpg.rf.ca7266be54f4d4f5060ecc851e846791.jpg\n",
            "rs_00052.jpg.rf.448a29a6e61a566cdb75f1d4ec06d71a.jpg\n",
            "rs_00052.jpg.rf.e8d059a33ae45f6343d94b7ca20f13bc.jpg\n",
            "si_00000.jpg.rf.43d7bf5fecd4ba6432187f3af84874a6.jpg\n",
            "si_00000.jpg.rf.b5672e58e8df057e638f29cfa345687b.jpg\n",
            "si_00001.jpg.rf.cb8779cc251496760db08dc6203e0f51.jpg\n",
            "si_00001.jpg.rf.d9093b12ebae244366933391de7e371d.jpg\n",
            "si_00002.jpg.rf.1ca0b649722d698060dfd60d565db381.jpg\n",
            "si_00002.jpg.rf.7d22c2070b6fd1bb6ece55ca4f7c6441.jpg\n",
            "si_00003.jpg.rf.16c4a5ebdec07b189a4c59c8e99b4dba.jpg\n",
            "si_00003.jpg.rf.77ad5e9055ab536d8e7f62ebdde41490.jpg\n",
            "si_00004.jpg.rf.1f436f4ead8a7db244aaff13d8a68c72.jpg\n",
            "si_00004.jpg.rf.2241aa123adba92a0a5b59091924ea23.jpg\n",
            "si_00005.jpg.rf.5e7a2b6b6843a0bfd69d54597a4fe184.jpg\n",
            "si_00005.jpg.rf.b0c5e561acf4ffd231a9158999fb27df.jpg\n",
            "si_00006.jpg.rf.5ffa0800d472076907c6675cce17169a.jpg\n",
            "si_00006.jpg.rf.61b093c9097c1bd714f71be4ba271c21.jpg\n",
            "si_00007.jpg.rf.59426fd493bfa4bd23d6fe1a7088cb22.jpg\n",
            "si_00007.jpg.rf.fc7a7410de6439f5e9fa88c583817f4a.jpg\n",
            "si_00008.jpg.rf.03df088411a5f7aea1479b3917457942.jpg\n",
            "si_00008.jpg.rf.7c47dbcb49b08eece7caf7649047707b.jpg\n",
            "si_00009.jpg.rf.2f5f6187af9759463f84e704f5b3f8db.jpg\n",
            "si_00009.jpg.rf.7e1875ef74b513a0d518c52844c2f31b.jpg\n",
            "si_00010.jpg.rf.18c054f1092c806c75661d83804ed3a8.jpg\n",
            "si_00010.jpg.rf.ec31fe65a884ffde1be8a6039ec67d30.jpg\n",
            "si_00011.jpg.rf.c3ce8d22c3dc6ea28c7ec64a269e6ac0.jpg\n",
            "si_00011.jpg.rf.f18aeca04c45e4bfc9076d54f2811084.jpg\n",
            "si_00012.jpg.rf.44ca5e4aa3760b7c9c50a646a80328ce.jpg\n",
            "si_00012.jpg.rf.d88ecafa030290416571187a1acc1b6b.jpg\n",
            "si_00013.jpg.rf.b00500e2b5b232a12b2ece00b9533878.jpg\n",
            "si_00013.jpg.rf.e0107c6dc0371cef9cdb874f41d9af46.jpg\n",
            "si_00014.jpg.rf.12ef6b840f65eb828bdfd8a61d9e5018.jpg\n",
            "si_00014.jpg.rf.900293ff121d2bde7b5b624aa925ceb7.jpg\n",
            "si_00015.jpg.rf.758d1ed5e5c061e956d7bae93d515576.jpg\n",
            "si_00015.jpg.rf.f79b0e5650157c061550b0e97a7e55ef.jpg\n",
            "si_00016.jpg.rf.b1dd950f00aaf980bb896941b5e30aee.jpg\n",
            "si_00016.jpg.rf.b4279f02a89be3b75ce1c891a1b6da74.jpg\n",
            "si_00017.jpg.rf.16904c7359d026f5f4a1ec8420fdd93c.jpg\n",
            "si_00017.jpg.rf.9c6a128a2f372e9ee72677cfd7ad31f2.jpg\n",
            "si_00018.jpg.rf.0bb4db7bd841d1b871cf6532a0b5586b.jpg\n",
            "si_00018.jpg.rf.c431abac60ae55bc4692aa06b8cf1f01.jpg\n",
            "si_00019.jpg.rf.9a39beb07496182c8a9a02409c752481.jpg\n",
            "si_00019.jpg.rf.d053a9bd017a06eeb4087681fbdce657.jpg\n",
            "si_00020.jpg.rf.38799897123a17bebfd8a3baa9a71bb1.jpg\n",
            "si_00020.jpg.rf.8b0b0e812e47f28fcd828342238e9efb.jpg\n",
            "si_00021.jpg.rf.679025d3238207bb0b0ee0a62eccad8d.jpg\n",
            "si_00021.jpg.rf.ae492dc89c1e3500f5cd38979417b510.jpg\n",
            "train_bottleneck.py\n",
            "train.py\n",
            "voc_annotation.py\n",
            "\u001b[01;34myolo3\u001b[0m/\n",
            "yolo.py\n",
            "yolov3.cfg\n",
            "yolov3-tiny.cfg\n",
            "yolo_video.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJBgkeSH7CQr",
        "colab_type": "text"
      },
      "source": [
        "# Set up and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYsK8KeC7GRs",
        "colab_type": "code",
        "outputId": "e3fcf733-5d89-4100-8e74-804057d09a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# download DarkNet weights \n",
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-16 18:39:31--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: yolov3.weights\n",
            "\n",
            "yolov3.weights      100%[===================>] 236.52M  19.6MB/s    in 13s     \n",
            "\n",
            "2020-02-16 18:39:44 (18.1 MB/s) - yolov3.weights saved [248007048/248007048]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSeTaCge7PdW",
        "colab_type": "code",
        "outputId": "c1b88544-c703-4534-a8b0-8349075bfa9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# call a Python script to set up our architecture with downloaded pre-trained weights\n",
        "!python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "Loading weights.\n",
            "Weights Header:  0 2 0 [32013312]\n",
            "Parsing Darknet config.\n",
            "Creating Keras model.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "Parsing section net_0\n",
            "Parsing section convolutional_0\n",
            "conv2d bn leaky (3, 3, 3, 32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2020-02-16 18:39:52.998698: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
            "2020-02-16 18:39:53.026038: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000165000 Hz\n",
            "2020-02-16 18:39:53.069645: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3182bc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-02-16 18:39:53.069683: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-02-16 18:39:53.079384: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-02-16 18:39:53.323711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-02-16 18:39:53.324413: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3182d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-02-16 18:39:53.324440: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-02-16 18:39:53.325506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-02-16 18:39:53.326035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-02-16 18:39:53.346902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-02-16 18:39:53.645609: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-02-16 18:39:53.784541: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-02-16 18:39:53.807961: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-02-16 18:39:54.138685: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-02-16 18:39:54.168277: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-02-16 18:39:54.725839: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-02-16 18:39:54.726024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-02-16 18:39:54.726708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-02-16 18:39:54.727197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-02-16 18:39:54.733016: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-02-16 18:39:54.734102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-02-16 18:39:54.734129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-02-16 18:39:54.734139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-02-16 18:39:54.735489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-02-16 18:39:54.736122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-02-16 18:39:54.736688: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-02-16 18:39:54.736725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "Parsing section convolutional_1\n",
            "conv2d bn leaky (3, 3, 32, 64)\n",
            "Parsing section convolutional_2\n",
            "conv2d bn leaky (1, 1, 64, 32)\n",
            "Parsing section convolutional_3\n",
            "conv2d bn leaky (3, 3, 32, 64)\n",
            "Parsing section shortcut_0\n",
            "Parsing section convolutional_4\n",
            "conv2d bn leaky (3, 3, 64, 128)\n",
            "Parsing section convolutional_5\n",
            "conv2d bn leaky (1, 1, 128, 64)\n",
            "Parsing section convolutional_6\n",
            "conv2d bn leaky (3, 3, 64, 128)\n",
            "Parsing section shortcut_1\n",
            "Parsing section convolutional_7\n",
            "conv2d bn leaky (1, 1, 128, 64)\n",
            "Parsing section convolutional_8\n",
            "conv2d bn leaky (3, 3, 64, 128)\n",
            "Parsing section shortcut_2\n",
            "Parsing section convolutional_9\n",
            "conv2d bn leaky (3, 3, 128, 256)\n",
            "Parsing section convolutional_10\n",
            "conv2d bn leaky (1, 1, 256, 128)\n",
            "Parsing section convolutional_11\n",
            "conv2d bn leaky (3, 3, 128, 256)\n",
            "Parsing section shortcut_3\n",
            "Parsing section convolutional_12\n",
            "conv2d bn leaky (1, 1, 256, 128)\n",
            "Parsing section convolutional_13\n",
            "conv2d bn leaky (3, 3, 128, 256)\n",
            "Parsing section shortcut_4\n",
            "Parsing section convolutional_14\n",
            "conv2d bn leaky (1, 1, 256, 128)\n",
            "Parsing section convolutional_15\n",
            "conv2d bn leaky (3, 3, 128, 256)\n",
            "Parsing section shortcut_5\n",
            "Parsing section convolutional_16\n",
            "conv2d bn leaky (1, 1, 256, 128)\n",
            "Parsing section convolutional_17\n",
            "conv2d bn leaky (3, 3, 128, 256)\n",
            "Parsing section shortcut_6\n",
            "Parsing section convolutional_18\n",
            "conv2d bn leaky (1, 1, 256, 128)\n",
            "Parsing section convolutional_19\n",
            "conv2d bn leaky (3, 3, 128, 256)\n",
            "Parsing section shortcut_7\n",
            "Parsing section convolutional_20\n",
            "conv2d bn leaky (1, 1, 256, 128)\n",
            "Parsing section convolutional_21\n",
            "conv2d bn leaky (3, 3, 128, 256)\n",
            "Parsing section shortcut_8\n",
            "Parsing section convolutional_22\n",
            "conv2d bn leaky (1, 1, 256, 128)\n",
            "Parsing section convolutional_23\n",
            "conv2d bn leaky (3, 3, 128, 256)\n",
            "Parsing section shortcut_9\n",
            "Parsing section convolutional_24\n",
            "conv2d bn leaky (1, 1, 256, 128)\n",
            "Parsing section convolutional_25\n",
            "conv2d bn leaky (3, 3, 128, 256)\n",
            "Parsing section shortcut_10\n",
            "Parsing section convolutional_26\n",
            "conv2d bn leaky (3, 3, 256, 512)\n",
            "Parsing section convolutional_27\n",
            "conv2d bn leaky (1, 1, 512, 256)\n",
            "Parsing section convolutional_28\n",
            "conv2d bn leaky (3, 3, 256, 512)\n",
            "Parsing section shortcut_11\n",
            "Parsing section convolutional_29\n",
            "conv2d bn leaky (1, 1, 512, 256)\n",
            "Parsing section convolutional_30\n",
            "conv2d bn leaky (3, 3, 256, 512)\n",
            "Parsing section shortcut_12\n",
            "Parsing section convolutional_31\n",
            "conv2d bn leaky (1, 1, 512, 256)\n",
            "Parsing section convolutional_32\n",
            "conv2d bn leaky (3, 3, 256, 512)\n",
            "Parsing section shortcut_13\n",
            "Parsing section convolutional_33\n",
            "conv2d bn leaky (1, 1, 512, 256)\n",
            "Parsing section convolutional_34\n",
            "conv2d bn leaky (3, 3, 256, 512)\n",
            "Parsing section shortcut_14\n",
            "Parsing section convolutional_35\n",
            "conv2d bn leaky (1, 1, 512, 256)\n",
            "Parsing section convolutional_36\n",
            "conv2d bn leaky (3, 3, 256, 512)\n",
            "Parsing section shortcut_15\n",
            "Parsing section convolutional_37\n",
            "conv2d bn leaky (1, 1, 512, 256)\n",
            "Parsing section convolutional_38\n",
            "conv2d bn leaky (3, 3, 256, 512)\n",
            "Parsing section shortcut_16\n",
            "Parsing section convolutional_39\n",
            "conv2d bn leaky (1, 1, 512, 256)\n",
            "Parsing section convolutional_40\n",
            "conv2d bn leaky (3, 3, 256, 512)\n",
            "Parsing section shortcut_17\n",
            "Parsing section convolutional_41\n",
            "conv2d bn leaky (1, 1, 512, 256)\n",
            "Parsing section convolutional_42\n",
            "conv2d bn leaky (3, 3, 256, 512)\n",
            "Parsing section shortcut_18\n",
            "Parsing section convolutional_43\n",
            "conv2d bn leaky (3, 3, 512, 1024)\n",
            "Parsing section convolutional_44\n",
            "conv2d bn leaky (1, 1, 1024, 512)\n",
            "Parsing section convolutional_45\n",
            "conv2d bn leaky (3, 3, 512, 1024)\n",
            "Parsing section shortcut_19\n",
            "Parsing section convolutional_46\n",
            "conv2d bn leaky (1, 1, 1024, 512)\n",
            "Parsing section convolutional_47\n",
            "conv2d bn leaky (3, 3, 512, 1024)\n",
            "Parsing section shortcut_20\n",
            "Parsing section convolutional_48\n",
            "conv2d bn leaky (1, 1, 1024, 512)\n",
            "Parsing section convolutional_49\n",
            "conv2d bn leaky (3, 3, 512, 1024)\n",
            "Parsing section shortcut_21\n",
            "Parsing section convolutional_50\n",
            "conv2d bn leaky (1, 1, 1024, 512)\n",
            "Parsing section convolutional_51\n",
            "conv2d bn leaky (3, 3, 512, 1024)\n",
            "Parsing section shortcut_22\n",
            "Parsing section convolutional_52\n",
            "conv2d bn leaky (1, 1, 1024, 512)\n",
            "Parsing section convolutional_53\n",
            "conv2d bn leaky (3, 3, 512, 1024)\n",
            "Parsing section convolutional_54\n",
            "conv2d bn leaky (1, 1, 1024, 512)\n",
            "Parsing section convolutional_55\n",
            "conv2d bn leaky (3, 3, 512, 1024)\n",
            "Parsing section convolutional_56\n",
            "conv2d bn leaky (1, 1, 1024, 512)\n",
            "Parsing section convolutional_57\n",
            "conv2d bn leaky (3, 3, 512, 1024)\n",
            "Parsing section convolutional_58\n",
            "conv2d    linear (1, 1, 1024, 255)\n",
            "Parsing section yolo_0\n",
            "Parsing section route_0\n",
            "Parsing section convolutional_59\n",
            "conv2d bn leaky (1, 1, 512, 256)\n",
            "Parsing section upsample_0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "Parsing section route_1\n",
            "Concatenating route layers: [<tf.Tensor 'up_sampling2d_1/ResizeNearestNeighbor:0' shape=(?, ?, ?, 256) dtype=float32>, <tf.Tensor 'add_19/add:0' shape=(?, ?, ?, 512) dtype=float32>]\n",
            "Parsing section convolutional_60\n",
            "conv2d bn leaky (1, 1, 768, 256)\n",
            "Parsing section convolutional_61\n",
            "conv2d bn leaky (3, 3, 256, 512)\n",
            "Parsing section convolutional_62\n",
            "conv2d bn leaky (1, 1, 512, 256)\n",
            "Parsing section convolutional_63\n",
            "conv2d bn leaky (3, 3, 256, 512)\n",
            "Parsing section convolutional_64\n",
            "conv2d bn leaky (1, 1, 512, 256)\n",
            "Parsing section convolutional_65\n",
            "conv2d bn leaky (3, 3, 256, 512)\n",
            "Parsing section convolutional_66\n",
            "conv2d    linear (1, 1, 512, 255)\n",
            "Parsing section yolo_1\n",
            "Parsing section route_2\n",
            "Parsing section convolutional_67\n",
            "conv2d bn leaky (1, 1, 256, 128)\n",
            "Parsing section upsample_1\n",
            "Parsing section route_3\n",
            "Concatenating route layers: [<tf.Tensor 'up_sampling2d_2/ResizeNearestNeighbor:0' shape=(?, ?, ?, 128) dtype=float32>, <tf.Tensor 'add_11/add:0' shape=(?, ?, ?, 256) dtype=float32>]\n",
            "Parsing section convolutional_68\n",
            "conv2d bn leaky (1, 1, 384, 128)\n",
            "Parsing section convolutional_69\n",
            "conv2d bn leaky (3, 3, 128, 256)\n",
            "Parsing section convolutional_70\n",
            "conv2d bn leaky (1, 1, 256, 128)\n",
            "Parsing section convolutional_71\n",
            "conv2d bn leaky (3, 3, 128, 256)\n",
            "Parsing section convolutional_72\n",
            "conv2d bn leaky (1, 1, 256, 128)\n",
            "Parsing section convolutional_73\n",
            "conv2d bn leaky (3, 3, 128, 256)\n",
            "Parsing section convolutional_74\n",
            "conv2d    linear (1, 1, 256, 255)\n",
            "Parsing section yolo_2\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, None, None, 3 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, None, 3 128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, None, None, 3 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, None, None, 3 0           leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, None, None, 6 18432       zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, None, 6 256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, None, None, 3 2048        leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, None, 3 128         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, None, None, 3 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, None, None, 6 18432       leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, None, None, 6 256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, None, None, 6 0           leaky_re_lu_2[0][0]              \n",
            "                                                                 leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPadding2D (None, None, None, 6 0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, None, None, 1 73728       zero_padding2d_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, None, None, 1 512         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, None, None, 6 8192        leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, None, None, 6 256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, None, None, 1 73728       leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, None, None, 1 512         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, None, None, 1 0           leaky_re_lu_5[0][0]              \n",
            "                                                                 leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, None, None, 6 8192        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, None, None, 6 256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, None, None, 1 73728       leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, None, None, 1 512         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, None, None, 1 0           add_2[0][0]                      \n",
            "                                                                 leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_3 (ZeroPadding2D (None, None, None, 1 0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, None, None, 2 294912      zero_padding2d_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, None, None, 2 1024        conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, None, None, 1 512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, None, None, 2 1024        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, None, None, 2 0           leaky_re_lu_10[0][0]             \n",
            "                                                                 leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, None, None, 1 32768       add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, None, None, 1 512         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, None, None, 2 1024        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, None, None, 2 0           add_4[0][0]                      \n",
            "                                                                 leaky_re_lu_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, None, None, 1 32768       add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, None, None, 1 512         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, None, None, 2 1024        conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, None, None, 2 0           add_5[0][0]                      \n",
            "                                                                 leaky_re_lu_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, None, None, 1 32768       add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, None, None, 1 512         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, None, None, 2 1024        conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, None, None, 2 0           add_6[0][0]                      \n",
            "                                                                 leaky_re_lu_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, None, None, 1 32768       add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, None, None, 1 512         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, None, None, 2 1024        conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, None, None, 2 0           add_7[0][0]                      \n",
            "                                                                 leaky_re_lu_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, None, None, 1 32768       add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, None, None, 1 512         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, None, None, 2 1024        conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_22 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, None, None, 2 0           add_8[0][0]                      \n",
            "                                                                 leaky_re_lu_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, None, None, 1 32768       add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, None, None, 1 512         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_23 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, None, None, 2 1024        conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_24 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, None, None, 2 0           add_9[0][0]                      \n",
            "                                                                 leaky_re_lu_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, None, None, 1 32768       add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, None, None, 1 512         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_25 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, None, None, 2 1024        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_26 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, None, None, 2 0           add_10[0][0]                     \n",
            "                                                                 leaky_re_lu_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_4 (ZeroPadding2D (None, None, None, 2 0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, None, None, 5 1179648     zero_padding2d_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, None, None, 5 2048        conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_27 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, None, None, 2 1024        conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_28 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, None, None, 5 2048        conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_29 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, None, None, 5 0           leaky_re_lu_27[0][0]             \n",
            "                                                                 leaky_re_lu_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, None, None, 2 131072      add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, None, None, 2 1024        conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_30 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, None, None, 5 2048        conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_31 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, None, None, 5 0           add_12[0][0]                     \n",
            "                                                                 leaky_re_lu_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, None, None, 2 131072      add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, None, None, 2 1024        conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_32 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, None, None, 5 2048        conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_33 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, None, None, 5 0           add_13[0][0]                     \n",
            "                                                                 leaky_re_lu_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, None, None, 2 131072      add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, None, None, 2 1024        conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_34 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, None, None, 5 2048        conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_35 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, None, None, 5 0           add_14[0][0]                     \n",
            "                                                                 leaky_re_lu_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, None, None, 2 131072      add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, None, None, 2 1024        conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_36 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, None, None, 5 2048        conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_37 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, None, None, 5 0           add_15[0][0]                     \n",
            "                                                                 leaky_re_lu_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, None, None, 2 131072      add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, None, None, 2 1024        conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_38 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, None, None, 5 2048        conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_39 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, None, None, 5 0           add_16[0][0]                     \n",
            "                                                                 leaky_re_lu_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, None, None, 2 131072      add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, None, None, 2 1024        conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_40 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, None, None, 5 2048        conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_41 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, None, None, 5 0           add_17[0][0]                     \n",
            "                                                                 leaky_re_lu_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, None, None, 2 131072      add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, None, None, 2 1024        conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_42 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, None, None, 5 2048        conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_43 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, None, None, 5 0           add_18[0][0]                     \n",
            "                                                                 leaky_re_lu_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_5 (ZeroPadding2D (None, None, None, 5 0           add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, None, None, 1 4718592     zero_padding2d_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, None, None, 1 4096        conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_44 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, None, None, 5 524288      leaky_re_lu_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, None, None, 5 2048        conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_45 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, None, None, 1 4096        conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_46 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, None, None, 1 0           leaky_re_lu_44[0][0]             \n",
            "                                                                 leaky_re_lu_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, None, None, 5 524288      add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, None, None, 5 2048        conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_47 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, None, None, 1 4096        conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_48 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, None, None, 1 0           add_20[0][0]                     \n",
            "                                                                 leaky_re_lu_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, None, None, 5 524288      add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, None, None, 5 2048        conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_49 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_49[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, None, None, 1 4096        conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_50 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, None, None, 1 0           add_21[0][0]                     \n",
            "                                                                 leaky_re_lu_50[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, None, None, 5 524288      add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, None, None, 5 2048        conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_51 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_51[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, None, None, 1 4096        conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_52 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, None, None, 1 0           add_22[0][0]                     \n",
            "                                                                 leaky_re_lu_52[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, None, None, 5 524288      add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, None, None, 5 2048        conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_53 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_53[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, None, None, 1 4096        conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_54 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, None, None, 5 524288      leaky_re_lu_54[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, None, None, 5 2048        conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_55 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_55[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, None, None, 1 4096        conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_56 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, None, None, 5 524288      leaky_re_lu_56[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, None, None, 5 2048        conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_57 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_57[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, None, None, 2 1024        conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_59 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, None, None, 2 0           leaky_re_lu_59[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, None, None, 7 0           up_sampling2d_1[0][0]            \n",
            "                                                                 add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, None, None, 2 196608      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, None, None, 2 1024        conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_60 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_60[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, None, None, 5 2048        conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_61 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_61[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, None, None, 2 1024        conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_62 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_62[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, None, None, 5 2048        conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_63 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_63[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, None, None, 2 1024        conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_64 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_64[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, None, None, 1 512         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_66 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, None, None, 1 0           leaky_re_lu_66[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, None, None, 3 0           up_sampling2d_2[0][0]            \n",
            "                                                                 add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, None, None, 1 49152       concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, None, None, 1 512         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_67 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_67[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, None, None, 2 1024        conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_68 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_68[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, None, None, 1 512         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_69 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_69[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, None, None, 2 1024        conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_70 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_70[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, None, None, 1 512         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_71 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_57[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_64[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_71[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, None, None, 1 4096        conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, None, None, 5 2048        conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, None, None, 2 1024        conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_58 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_65 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_72 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, None, None, 2 261375      leaky_re_lu_58[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, None, None, 2 130815      leaky_re_lu_65[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, None, None, 2 65535       leaky_re_lu_72[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 62,001,757\n",
            "Trainable params: 61,949,149\n",
            "Non-trainable params: 52,608\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Saved Keras model to model_data/yolo.h5\n",
            "Read 62001757 of 62001757.0 from Darknet weights.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hBFndz8VeI6",
        "colab_type": "code",
        "outputId": "442f243d-2f66-41b1-a5c3-13daa14c44ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"\n",
        "Self-contained Python script to train YOLOv3 on your own dataset\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.layers import Input, Lambda\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n",
        "from yolo3.utils import get_random_data\n",
        "\n",
        "\n",
        "def _main():\n",
        "    annotation_path = '_annotations.txt'  # path to Roboflow data annotations\n",
        "    log_dir = 'logs/000/'                 # where we're storing our logs\n",
        "    classes_path = '_classes.txt'         # path to Roboflow class names\n",
        "    anchors_path = 'model_data/yolo_anchors.txt'\n",
        "    class_names = get_classes(classes_path)\n",
        "    print(\"-------------------CLASS NAMES-------------------\")\n",
        "    print(class_names)\n",
        "    print(\"-------------------CLASS NAMES-------------------\")\n",
        "    num_classes = len(class_names)\n",
        "    anchors = get_anchors(anchors_path)\n",
        "\n",
        "    input_shape = (416,416) # multiple of 32, hw\n",
        "\n",
        "    is_tiny_version = len(anchors)==6 # default setting\n",
        "    if is_tiny_version:\n",
        "        model = create_tiny_model(input_shape, anchors, num_classes,\n",
        "            freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')\n",
        "    else:\n",
        "        model = create_model(input_shape, anchors, num_classes,\n",
        "            freeze_body=2, weights_path='model_data/yolo.h5') # make sure you know what you freeze\n",
        "\n",
        "    logging = TensorBoard(log_dir=log_dir)\n",
        "    checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
        "        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
        "\n",
        "    val_split = 0.2 # set the size of the validation set\n",
        "    with open(annotation_path) as f:\n",
        "        lines = f.readlines()\n",
        "    np.random.seed(10101)\n",
        "    np.random.shuffle(lines)\n",
        "    np.random.seed(None)\n",
        "    num_val = int(len(lines)*val_split)\n",
        "    num_train = len(lines) - num_val\n",
        "\n",
        "    # Train with frozen layers first, to get a stable loss.\n",
        "    # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n",
        "    if True:\n",
        "        model.compile(optimizer=Adam(lr=1e-3), loss={\n",
        "            # use custom yolo_loss Lambda layer.\n",
        "            'yolo_loss': lambda y_true, y_pred: y_pred})\n",
        "\n",
        "        batch_size = 16\n",
        "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
        "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
        "                steps_per_epoch=max(1, num_train//batch_size),\n",
        "                validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
        "                validation_steps=max(1, num_val//batch_size),\n",
        "                epochs=500,\n",
        "                initial_epoch=0,\n",
        "                callbacks=[logging, checkpoint])\n",
        "        model.save_weights(log_dir + 'trained_weights_stage_1.h5')\n",
        "\n",
        "    # Unfreeze and continue training, to fine-tune.\n",
        "    # Train longer if the result is not good.\n",
        "    if True:\n",
        "        for i in range(len(model.layers)):\n",
        "            model.layers[i].trainable = True\n",
        "        model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
        "        print('Unfreeze all of the layers.')\n",
        "\n",
        "        batch_size = 16 # note that more GPU memory is required after unfreezing the body\n",
        "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
        "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
        "            steps_per_epoch=max(1, num_train//batch_size),\n",
        "            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
        "            validation_steps=max(1, num_val//batch_size),\n",
        "            epochs=100,\n",
        "            initial_epoch=50,\n",
        "            callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n",
        "        model.save_weights(log_dir + 'trained_weights_final.h5')\n",
        "\n",
        "    # Further training if needed.\n",
        "\n",
        "\n",
        "def get_classes(classes_path):\n",
        "    '''loads the classes'''\n",
        "    with open(classes_path) as f:\n",
        "        class_names = f.readlines()\n",
        "    class_names = [c.strip() for c in class_names]\n",
        "    return class_names\n",
        "\n",
        "def get_anchors(anchors_path):\n",
        "    '''loads the anchors from a file'''\n",
        "    with open(anchors_path) as f:\n",
        "        anchors = f.readline()\n",
        "    anchors = [float(x) for x in anchors.split(',')]\n",
        "    return np.array(anchors).reshape(-1, 2)\n",
        "\n",
        "\n",
        "def create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
        "            weights_path='model_data/yolo.h5'):\n",
        "    '''create the training model'''\n",
        "    K.clear_session() # get a new session\n",
        "    image_input = Input(shape=(None, None, 3))\n",
        "    h, w = input_shape\n",
        "    num_anchors = len(anchors)\n",
        "\n",
        "    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n",
        "        num_anchors//3, num_classes+5)) for l in range(3)]\n",
        "\n",
        "    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n",
        "    print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
        "\n",
        "    if load_pretrained:\n",
        "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
        "        print('Load weights {}.'.format(weights_path))\n",
        "        if freeze_body in [1, 2]:\n",
        "            # Freeze darknet53 body or freeze all but 3 output layers.\n",
        "            num = (185, len(model_body.layers)-3)[freeze_body-1]\n",
        "            for i in range(num): model_body.layers[i].trainable = False\n",
        "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
        "\n",
        "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
        "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})(\n",
        "        [*model_body.output, *y_true])\n",
        "    model = Model([model_body.input, *y_true], model_loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_tiny_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
        "            weights_path='model_data/tiny_yolo_weights.h5'):\n",
        "    '''create the training model, for Tiny YOLOv3'''\n",
        "    K.clear_session() # get a new session\n",
        "    image_input = Input(shape=(None, None, 3))\n",
        "    h, w = input_shape\n",
        "    num_anchors = len(anchors)\n",
        "\n",
        "    y_true = [Input(shape=(h//{0:32, 1:16}[l], w//{0:32, 1:16}[l], \\\n",
        "        num_anchors//2, num_classes+5)) for l in range(2)]\n",
        "\n",
        "    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n",
        "    print('Create Tiny YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
        "\n",
        "    if load_pretrained:\n",
        "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
        "        print('Load weights {}.'.format(weights_path))\n",
        "        if freeze_body in [1, 2]:\n",
        "            # Freeze the darknet body or freeze all but 2 output layers.\n",
        "            num = (20, len(model_body.layers)-2)[freeze_body-1]\n",
        "            for i in range(num): model_body.layers[i].trainable = False\n",
        "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
        "\n",
        "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
        "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.7})(\n",
        "        [*model_body.output, *y_true])\n",
        "    model = Model([model_body.input, *y_true], model_loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
        "    '''data generator for fit_generator'''\n",
        "    n = len(annotation_lines)\n",
        "    i = 0\n",
        "    while True:\n",
        "        image_data = []\n",
        "        box_data = []\n",
        "        for b in range(batch_size):\n",
        "            if i==0:\n",
        "                np.random.shuffle(annotation_lines)\n",
        "            image, box = get_random_data(annotation_lines[i], input_shape, random=True)\n",
        "            image_data.append(image)\n",
        "            box_data.append(box)\n",
        "            i = (i+1) % n\n",
        "        image_data = np.array(image_data)\n",
        "        box_data = np.array(box_data)\n",
        "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
        "        yield [image_data, *y_true], np.zeros(batch_size)\n",
        "\n",
        "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
        "    n = len(annotation_lines)\n",
        "    if n==0 or batch_size<=0: return None\n",
        "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    _main()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-------------------CLASS NAMES-------------------\n",
            "['ch', 'de', 'es', 'fr', 'gr', 'hu', 'me', 'pt', 'rs', 'si']\n",
            "-------------------CLASS NAMES-------------------\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "Create YOLOv3 model with 9 anchors and 10 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((1, 1, 1024, 45) vs (255, 1024, 1, 1)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((45,) vs (255,)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((1, 1, 512, 45) vs (255, 512, 1, 1)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((45,) vs (255,)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((1, 1, 256, 45) vs (255, 256, 1, 1)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((45,) vs (255,)).\n",
            "  weight_values[i].shape))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Load weights model_data/yolo.h5.\n",
            "Freeze the first 249 layers of total 252 layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1521: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3080: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Train on 364 samples, val on 90 samples, with batch size 16.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/500\n",
            "22/22 [==============================] - 36s 2s/step - loss: 3348.8184 - val_loss: 925.4442\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:995: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "Epoch 2/500\n",
            "22/22 [==============================] - 25s 1s/step - loss: 486.8211 - val_loss: 298.2726\n",
            "Epoch 3/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 235.7451 - val_loss: 185.8925\n",
            "Epoch 4/500\n",
            "22/22 [==============================] - 15s 691ms/step - loss: 166.4183 - val_loss: 163.9651\n",
            "Epoch 5/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 135.4110 - val_loss: 129.4671\n",
            "Epoch 6/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 111.6152 - val_loss: 103.8283\n",
            "Epoch 7/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 96.2871 - val_loss: 88.4312\n",
            "Epoch 8/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 84.9742 - val_loss: 84.2861\n",
            "Epoch 9/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 75.4400 - val_loss: 74.1896\n",
            "Epoch 10/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 70.9537 - val_loss: 65.6658\n",
            "Epoch 11/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 65.6035 - val_loss: 61.1472\n",
            "Epoch 12/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 59.0617 - val_loss: 57.2807\n",
            "Epoch 13/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 56.6189 - val_loss: 52.0343\n",
            "Epoch 14/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 51.6892 - val_loss: 53.2544\n",
            "Epoch 15/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 51.1820 - val_loss: 46.3045\n",
            "Epoch 16/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 47.0693 - val_loss: 46.3571\n",
            "Epoch 17/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 47.2062 - val_loss: 45.5944\n",
            "Epoch 18/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 44.0081 - val_loss: 42.3059\n",
            "Epoch 19/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 43.1018 - val_loss: 41.1936\n",
            "Epoch 20/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 41.2045 - val_loss: 40.7106\n",
            "Epoch 21/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 41.1150 - val_loss: 40.1583\n",
            "Epoch 22/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 39.3651 - val_loss: 37.8864\n",
            "Epoch 23/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 36.5375 - val_loss: 35.6888\n",
            "Epoch 24/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 36.7649 - val_loss: 36.9183\n",
            "Epoch 25/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 35.7650 - val_loss: 33.1902\n",
            "Epoch 26/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 35.1993 - val_loss: 34.2259\n",
            "Epoch 27/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 34.8787 - val_loss: 34.3984\n",
            "Epoch 28/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 34.1595 - val_loss: 31.5746\n",
            "Epoch 29/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 33.6397 - val_loss: 33.9700\n",
            "Epoch 30/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 33.1360 - val_loss: 32.1824\n",
            "Epoch 31/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 32.5757 - val_loss: 29.7531\n",
            "Epoch 32/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 31.5799 - val_loss: 30.5668\n",
            "Epoch 33/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 31.0529 - val_loss: 31.6465\n",
            "Epoch 34/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 31.0379 - val_loss: 28.9841\n",
            "Epoch 35/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 31.3827 - val_loss: 30.6351\n",
            "Epoch 36/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 29.7375 - val_loss: 29.1420\n",
            "Epoch 37/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 30.3027 - val_loss: 29.0730\n",
            "Epoch 38/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 30.7614 - val_loss: 28.9686\n",
            "Epoch 39/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 28.8522 - val_loss: 28.7859\n",
            "Epoch 40/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 28.0510 - val_loss: 26.9808\n",
            "Epoch 41/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 27.7422 - val_loss: 26.6038\n",
            "Epoch 42/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 28.6576 - val_loss: 27.7052\n",
            "Epoch 43/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 28.4639 - val_loss: 26.1067\n",
            "Epoch 44/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 27.6873 - val_loss: 26.4306\n",
            "Epoch 45/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 28.4135 - val_loss: 26.6093\n",
            "Epoch 46/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 26.4998 - val_loss: 25.6628\n",
            "Epoch 47/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 27.2088 - val_loss: 26.3741\n",
            "Epoch 48/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 27.7204 - val_loss: 26.2163\n",
            "Epoch 49/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 26.4085 - val_loss: 26.4684\n",
            "Epoch 50/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 26.3748 - val_loss: 24.3533\n",
            "Epoch 51/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 26.4961 - val_loss: 26.0086\n",
            "Epoch 52/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 26.4621 - val_loss: 25.0326\n",
            "Epoch 53/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 26.2307 - val_loss: 25.3435\n",
            "Epoch 54/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 26.6353 - val_loss: 26.0042\n",
            "Epoch 55/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 25.6044 - val_loss: 25.1036\n",
            "Epoch 56/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 25.6582 - val_loss: 23.9290\n",
            "Epoch 57/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 26.0104 - val_loss: 24.5519\n",
            "Epoch 58/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 25.2597 - val_loss: 26.0665\n",
            "Epoch 59/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 25.6652 - val_loss: 24.2858\n",
            "Epoch 60/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 25.3687 - val_loss: 23.8603\n",
            "Epoch 61/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 25.1501 - val_loss: 25.9969\n",
            "Epoch 62/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 24.9105 - val_loss: 24.8608\n",
            "Epoch 63/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 25.3226 - val_loss: 23.2652\n",
            "Epoch 64/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 25.2195 - val_loss: 24.0938\n",
            "Epoch 65/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 24.2138 - val_loss: 24.8178\n",
            "Epoch 66/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 25.2065 - val_loss: 22.6571\n",
            "Epoch 67/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 24.3305 - val_loss: 24.2241\n",
            "Epoch 68/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 25.0069 - val_loss: 23.5010\n",
            "Epoch 69/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 24.1786 - val_loss: 24.0554\n",
            "Epoch 70/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 23.3616 - val_loss: 23.0744\n",
            "Epoch 71/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 24.3362 - val_loss: 24.8640\n",
            "Epoch 72/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 24.1179 - val_loss: 24.3559\n",
            "Epoch 73/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 24.1520 - val_loss: 23.4134\n",
            "Epoch 74/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 24.4832 - val_loss: 23.1447\n",
            "Epoch 75/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 24.3289 - val_loss: 23.4377\n",
            "Epoch 76/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 24.0469 - val_loss: 24.5238\n",
            "Epoch 77/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 24.6654 - val_loss: 21.2799\n",
            "Epoch 78/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 23.0000 - val_loss: 23.9710\n",
            "Epoch 79/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 23.2609 - val_loss: 22.4074\n",
            "Epoch 80/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 23.7189 - val_loss: 23.1890\n",
            "Epoch 81/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 23.5139 - val_loss: 23.3053\n",
            "Epoch 82/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 24.4210 - val_loss: 23.2051\n",
            "Epoch 83/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 23.2686 - val_loss: 23.1306\n",
            "Epoch 84/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 22.6240 - val_loss: 21.5674\n",
            "Epoch 85/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 23.5714 - val_loss: 22.2169\n",
            "Epoch 86/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 25.0178 - val_loss: 23.2843\n",
            "Epoch 87/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 22.8552 - val_loss: 22.3424\n",
            "Epoch 88/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 23.7328 - val_loss: 23.0860\n",
            "Epoch 89/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 22.3303 - val_loss: 21.8795\n",
            "Epoch 90/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 23.4586 - val_loss: 23.0777\n",
            "Epoch 91/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 23.5998 - val_loss: 22.5226\n",
            "Epoch 92/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 22.3020 - val_loss: 23.4451\n",
            "Epoch 93/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 23.3521 - val_loss: 22.7452\n",
            "Epoch 94/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 22.4480 - val_loss: 21.3401\n",
            "Epoch 95/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 22.3853 - val_loss: 21.5348\n",
            "Epoch 96/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 23.7406 - val_loss: 23.3093\n",
            "Epoch 97/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 22.4183 - val_loss: 21.0704\n",
            "Epoch 98/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 23.0806 - val_loss: 23.5843\n",
            "Epoch 99/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 22.2057 - val_loss: 21.9752\n",
            "Epoch 100/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 23.2066 - val_loss: 22.0970\n",
            "Epoch 101/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 22.5420 - val_loss: 21.0574\n",
            "Epoch 102/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 23.4833 - val_loss: 24.4285\n",
            "Epoch 103/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.2315 - val_loss: 20.8849\n",
            "Epoch 104/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 24.2811 - val_loss: 23.5852\n",
            "Epoch 105/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.8456 - val_loss: 21.5602\n",
            "Epoch 106/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 22.2909 - val_loss: 21.3972\n",
            "Epoch 107/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 23.0637 - val_loss: 22.7888\n",
            "Epoch 108/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 22.8475 - val_loss: 21.8848\n",
            "Epoch 109/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.1756 - val_loss: 22.2506\n",
            "Epoch 110/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 23.3739 - val_loss: 21.1376\n",
            "Epoch 111/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 22.4362 - val_loss: 22.9097\n",
            "Epoch 112/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.9861 - val_loss: 22.3437\n",
            "Epoch 113/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 23.0399 - val_loss: 21.1696\n",
            "Epoch 114/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 22.7036 - val_loss: 21.5900\n",
            "Epoch 115/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 22.4592 - val_loss: 21.4525\n",
            "Epoch 116/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.6183 - val_loss: 21.9905\n",
            "Epoch 117/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 23.3289 - val_loss: 22.0175\n",
            "Epoch 118/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.7491 - val_loss: 20.2959\n",
            "Epoch 119/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 22.5657 - val_loss: 22.8716\n",
            "Epoch 120/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 22.0284 - val_loss: 22.1132\n",
            "Epoch 121/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 22.1237 - val_loss: 20.6645\n",
            "Epoch 122/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.8688 - val_loss: 22.9668\n",
            "Epoch 123/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 22.0276 - val_loss: 19.4557\n",
            "Epoch 124/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.9176 - val_loss: 22.6015\n",
            "Epoch 125/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.9886 - val_loss: 22.5801\n",
            "Epoch 126/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.3496 - val_loss: 21.4227\n",
            "Epoch 127/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 23.1390 - val_loss: 21.2920\n",
            "Epoch 128/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 22.2628 - val_loss: 21.6495\n",
            "Epoch 129/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.3017 - val_loss: 22.7380\n",
            "Epoch 130/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.4393 - val_loss: 21.3717\n",
            "Epoch 131/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.7917 - val_loss: 20.0444\n",
            "Epoch 132/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 22.0077 - val_loss: 23.5852\n",
            "Epoch 133/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.9836 - val_loss: 20.3205\n",
            "Epoch 134/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 23.2030 - val_loss: 21.7755\n",
            "Epoch 135/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.3547 - val_loss: 22.3165\n",
            "Epoch 136/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 22.3343 - val_loss: 21.4285\n",
            "Epoch 137/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.5460 - val_loss: 21.3811\n",
            "Epoch 138/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 22.4650 - val_loss: 20.9480\n",
            "Epoch 139/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 22.0179 - val_loss: 21.3236\n",
            "Epoch 140/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.6611 - val_loss: 21.2951\n",
            "Epoch 141/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.4812 - val_loss: 20.4959\n",
            "Epoch 142/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 22.0068 - val_loss: 21.6886\n",
            "Epoch 143/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.2286 - val_loss: 21.5428\n",
            "Epoch 144/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.2472 - val_loss: 21.6897\n",
            "Epoch 145/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.1383 - val_loss: 21.8208\n",
            "Epoch 146/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.6878 - val_loss: 20.4855\n",
            "Epoch 147/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 22.2499 - val_loss: 21.0964\n",
            "Epoch 148/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.8671 - val_loss: 21.6295\n",
            "Epoch 149/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.6457 - val_loss: 19.5690\n",
            "Epoch 150/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.4017 - val_loss: 23.5226\n",
            "Epoch 151/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.8996 - val_loss: 20.8240\n",
            "Epoch 152/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.0555 - val_loss: 20.3109\n",
            "Epoch 153/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.4576 - val_loss: 21.6697\n",
            "Epoch 154/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.4639 - val_loss: 22.3294\n",
            "Epoch 155/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.3586 - val_loss: 19.8879\n",
            "Epoch 156/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.3322 - val_loss: 22.7846\n",
            "Epoch 157/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.8195 - val_loss: 20.5832\n",
            "Epoch 158/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.7905 - val_loss: 21.5130\n",
            "Epoch 159/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 22.0214 - val_loss: 20.4405\n",
            "Epoch 160/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.8292 - val_loss: 21.6045\n",
            "Epoch 161/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.4977 - val_loss: 19.9571\n",
            "Epoch 162/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.8018 - val_loss: 20.9535\n",
            "Epoch 163/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.9738 - val_loss: 20.9591\n",
            "Epoch 164/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.4939 - val_loss: 21.1537\n",
            "Epoch 165/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.6270 - val_loss: 20.1581\n",
            "Epoch 166/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.6089 - val_loss: 22.1296\n",
            "Epoch 167/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.4712 - val_loss: 19.5546\n",
            "Epoch 168/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.9479 - val_loss: 21.9064\n",
            "Epoch 169/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.1939 - val_loss: 20.2316\n",
            "Epoch 170/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.5888 - val_loss: 22.0357\n",
            "Epoch 171/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.2570 - val_loss: 21.4467\n",
            "Epoch 172/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.3760 - val_loss: 20.3849\n",
            "Epoch 173/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.7072 - val_loss: 20.9579\n",
            "Epoch 174/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.2469 - val_loss: 22.4913\n",
            "Epoch 175/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.1220 - val_loss: 21.5596\n",
            "Epoch 176/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.8407 - val_loss: 19.4534\n",
            "Epoch 177/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.7265 - val_loss: 22.4345\n",
            "Epoch 178/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.1596 - val_loss: 20.7358\n",
            "Epoch 179/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.3079 - val_loss: 21.7297\n",
            "Epoch 180/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.7651 - val_loss: 21.2313\n",
            "Epoch 181/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.0118 - val_loss: 21.5441\n",
            "Epoch 182/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.5196 - val_loss: 20.3728\n",
            "Epoch 183/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.3898 - val_loss: 22.6921\n",
            "Epoch 184/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.9401 - val_loss: 21.6470\n",
            "Epoch 185/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.9506 - val_loss: 18.6527\n",
            "Epoch 186/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.6018 - val_loss: 23.0465\n",
            "Epoch 187/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.8778 - val_loss: 20.1936\n",
            "Epoch 188/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.0106 - val_loss: 21.3239\n",
            "Epoch 189/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.1834 - val_loss: 20.1527\n",
            "Epoch 190/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.4351 - val_loss: 21.6123\n",
            "Epoch 191/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 22.1642 - val_loss: 18.6346\n",
            "Epoch 192/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.2843 - val_loss: 20.9898\n",
            "Epoch 193/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.9580 - val_loss: 18.6190\n",
            "Epoch 194/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.0062 - val_loss: 23.3653\n",
            "Epoch 195/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.7985 - val_loss: 19.8432\n",
            "Epoch 196/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.1299 - val_loss: 20.6971\n",
            "Epoch 197/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.3198 - val_loss: 20.2968\n",
            "Epoch 198/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.2245 - val_loss: 20.5929\n",
            "Epoch 199/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.8566 - val_loss: 21.4666\n",
            "Epoch 200/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.2223 - val_loss: 21.3524\n",
            "Epoch 201/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.5589 - val_loss: 20.3140\n",
            "Epoch 202/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.0729 - val_loss: 21.4384\n",
            "Epoch 203/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.7291 - val_loss: 20.0737\n",
            "Epoch 204/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.1259 - val_loss: 20.5816\n",
            "Epoch 205/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 22.1180 - val_loss: 20.7393\n",
            "Epoch 206/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.5977 - val_loss: 21.8427\n",
            "Epoch 207/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.9320 - val_loss: 19.7476\n",
            "Epoch 208/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.7483 - val_loss: 21.0861\n",
            "Epoch 209/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.6137 - val_loss: 21.1083\n",
            "Epoch 210/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.6352 - val_loss: 19.1421\n",
            "Epoch 211/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.3558 - val_loss: 21.5140\n",
            "Epoch 212/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.6840 - val_loss: 20.0389\n",
            "Epoch 213/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.8078 - val_loss: 20.2864\n",
            "Epoch 214/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.3147 - val_loss: 21.0749\n",
            "Epoch 215/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.0674 - val_loss: 19.4027\n",
            "Epoch 216/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.2744 - val_loss: 19.9468\n",
            "Epoch 217/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.0407 - val_loss: 20.9384\n",
            "Epoch 218/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.6688 - val_loss: 20.6247\n",
            "Epoch 219/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.0201 - val_loss: 19.8512\n",
            "Epoch 220/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.8395 - val_loss: 22.3390\n",
            "Epoch 221/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.4113 - val_loss: 18.6986\n",
            "Epoch 222/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.1241 - val_loss: 21.3156\n",
            "Epoch 223/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.1100 - val_loss: 20.2127\n",
            "Epoch 224/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.1409 - val_loss: 21.5007\n",
            "Epoch 225/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.3498 - val_loss: 21.2901\n",
            "Epoch 226/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.5393 - val_loss: 20.3132\n",
            "Epoch 227/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.3744 - val_loss: 19.7308\n",
            "Epoch 228/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.5556 - val_loss: 19.5238\n",
            "Epoch 229/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.4980 - val_loss: 20.3347\n",
            "Epoch 230/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.6693 - val_loss: 19.0420\n",
            "Epoch 231/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.5473 - val_loss: 22.6415\n",
            "Epoch 232/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.9146 - val_loss: 18.3329\n",
            "Epoch 233/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.9755 - val_loss: 21.4072\n",
            "Epoch 234/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.5038 - val_loss: 21.5558\n",
            "Epoch 235/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.5344 - val_loss: 20.5931\n",
            "Epoch 236/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.1698 - val_loss: 20.7280\n",
            "Epoch 237/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.4320 - val_loss: 19.2608\n",
            "Epoch 238/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.9805 - val_loss: 22.3353\n",
            "Epoch 239/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.6256 - val_loss: 18.0686\n",
            "Epoch 240/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.2817 - val_loss: 22.4692\n",
            "Epoch 241/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.9789 - val_loss: 19.4058\n",
            "Epoch 242/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.8098 - val_loss: 21.7057\n",
            "Epoch 243/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.1346 - val_loss: 18.9501\n",
            "Epoch 244/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.1993 - val_loss: 20.3237\n",
            "Epoch 245/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.7500 - val_loss: 20.6041\n",
            "Epoch 246/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.7173 - val_loss: 21.2589\n",
            "Epoch 247/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.4924 - val_loss: 19.9277\n",
            "Epoch 248/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.8235 - val_loss: 21.0568\n",
            "Epoch 249/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.8439 - val_loss: 18.3189\n",
            "Epoch 250/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.0842 - val_loss: 21.8085\n",
            "Epoch 251/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.3291 - val_loss: 20.8485\n",
            "Epoch 252/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.7212 - val_loss: 19.5461\n",
            "Epoch 253/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.2543 - val_loss: 20.4396\n",
            "Epoch 254/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.2742 - val_loss: 20.0535\n",
            "Epoch 255/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.7974 - val_loss: 19.8370\n",
            "Epoch 256/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.3334 - val_loss: 23.0190\n",
            "Epoch 257/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.8258 - val_loss: 19.6067\n",
            "Epoch 258/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.6759 - val_loss: 20.3446\n",
            "Epoch 259/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.1461 - val_loss: 20.3478\n",
            "Epoch 260/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.8974 - val_loss: 20.0475\n",
            "Epoch 261/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.0073 - val_loss: 21.2263\n",
            "Epoch 262/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.6680 - val_loss: 21.1963\n",
            "Epoch 263/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.5417 - val_loss: 19.6986\n",
            "Epoch 264/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.8134 - val_loss: 19.1771\n",
            "Epoch 265/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.5573 - val_loss: 21.3528\n",
            "Epoch 266/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.8463 - val_loss: 21.6542\n",
            "Epoch 267/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.3516 - val_loss: 18.6208\n",
            "Epoch 268/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.5040 - val_loss: 20.4761\n",
            "Epoch 269/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.7804 - val_loss: 19.8599\n",
            "Epoch 270/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.9176 - val_loss: 19.1640\n",
            "Epoch 271/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.1965 - val_loss: 19.6215\n",
            "Epoch 272/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.2081 - val_loss: 21.3017\n",
            "Epoch 273/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.2660 - val_loss: 21.7496\n",
            "Epoch 274/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.8225 - val_loss: 18.9955\n",
            "Epoch 275/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.4751 - val_loss: 19.2122\n",
            "Epoch 276/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.5905 - val_loss: 20.2643\n",
            "Epoch 277/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.2590 - val_loss: 21.7941\n",
            "Epoch 278/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.1103 - val_loss: 19.9086\n",
            "Epoch 279/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.8166 - val_loss: 21.3569\n",
            "Epoch 280/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.9063 - val_loss: 20.2344\n",
            "Epoch 281/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.2503 - val_loss: 20.9951\n",
            "Epoch 282/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.8508 - val_loss: 19.2292\n",
            "Epoch 283/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.9962 - val_loss: 21.2518\n",
            "Epoch 284/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.5675 - val_loss: 17.9269\n",
            "Epoch 285/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.9791 - val_loss: 20.3652\n",
            "Epoch 286/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.2246 - val_loss: 19.7623\n",
            "Epoch 287/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.5771 - val_loss: 20.6975\n",
            "Epoch 288/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.9842 - val_loss: 19.7192\n",
            "Epoch 289/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 17.8808 - val_loss: 20.3832\n",
            "Epoch 290/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.4779 - val_loss: 20.2583\n",
            "Epoch 291/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.8605 - val_loss: 20.1527\n",
            "Epoch 292/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.9141 - val_loss: 20.2248\n",
            "Epoch 293/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.5509 - val_loss: 19.5072\n",
            "Epoch 294/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.8257 - val_loss: 22.1947\n",
            "Epoch 295/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 18.9491 - val_loss: 19.4893\n",
            "Epoch 296/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.3604 - val_loss: 18.9208\n",
            "Epoch 297/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.7281 - val_loss: 20.4921\n",
            "Epoch 298/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.8990 - val_loss: 20.5650\n",
            "Epoch 299/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.4717 - val_loss: 19.9736\n",
            "Epoch 300/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.5375 - val_loss: 20.3069\n",
            "Epoch 301/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.5619 - val_loss: 20.2579\n",
            "Epoch 302/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.8920 - val_loss: 20.8780\n",
            "Epoch 303/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.9371 - val_loss: 18.7000\n",
            "Epoch 304/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.9137 - val_loss: 21.0215\n",
            "Epoch 305/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.7795 - val_loss: 18.6790\n",
            "Epoch 306/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.3810 - val_loss: 20.6344\n",
            "Epoch 307/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.1522 - val_loss: 18.5651\n",
            "Epoch 308/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.1982 - val_loss: 21.3975\n",
            "Epoch 309/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.8309 - val_loss: 21.1399\n",
            "Epoch 310/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.3396 - val_loss: 19.8860\n",
            "Epoch 311/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.6425 - val_loss: 21.0862\n",
            "Epoch 312/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.8150 - val_loss: 22.1430\n",
            "Epoch 313/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 18.7776 - val_loss: 18.7158\n",
            "Epoch 314/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.3543 - val_loss: 20.3868\n",
            "Epoch 315/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.1329 - val_loss: 20.7281\n",
            "Epoch 316/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.7882 - val_loss: 20.8059\n",
            "Epoch 317/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.8826 - val_loss: 19.0951\n",
            "Epoch 318/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.6732 - val_loss: 22.4611\n",
            "Epoch 319/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 18.7073 - val_loss: 20.0200\n",
            "Epoch 320/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 21.0517 - val_loss: 21.0873\n",
            "Epoch 321/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.3366 - val_loss: 20.1674\n",
            "Epoch 322/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 18.8175 - val_loss: 20.0952\n",
            "Epoch 323/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.4652 - val_loss: 21.2050\n",
            "Epoch 324/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.4619 - val_loss: 19.5127\n",
            "Epoch 325/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.0851 - val_loss: 19.4123\n",
            "Epoch 326/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.0856 - val_loss: 18.4564\n",
            "Epoch 327/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.8648 - val_loss: 21.3879\n",
            "Epoch 328/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.1853 - val_loss: 20.5195\n",
            "Epoch 329/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.9897 - val_loss: 20.6372\n",
            "Epoch 330/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.8970 - val_loss: 19.2717\n",
            "Epoch 331/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.4121 - val_loss: 20.9327\n",
            "Epoch 332/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.7391 - val_loss: 19.8076\n",
            "Epoch 333/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.0683 - val_loss: 21.1367\n",
            "Epoch 334/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.4257 - val_loss: 20.0942\n",
            "Epoch 335/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.7600 - val_loss: 20.9061\n",
            "Epoch 336/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.2753 - val_loss: 18.9814\n",
            "Epoch 337/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.8050 - val_loss: 21.2729\n",
            "Epoch 338/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.7980 - val_loss: 18.6559\n",
            "Epoch 339/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.6366 - val_loss: 20.1291\n",
            "Epoch 340/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.7394 - val_loss: 21.7920\n",
            "Epoch 341/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.7376 - val_loss: 19.0866\n",
            "Epoch 342/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.6740 - val_loss: 20.6590\n",
            "Epoch 343/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.2667 - val_loss: 19.8415\n",
            "Epoch 344/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.2654 - val_loss: 19.9116\n",
            "Epoch 345/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.0382 - val_loss: 22.0399\n",
            "Epoch 346/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.8531 - val_loss: 18.1801\n",
            "Epoch 347/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.1363 - val_loss: 21.2397\n",
            "Epoch 348/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.2353 - val_loss: 19.5509\n",
            "Epoch 349/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.3598 - val_loss: 20.3024\n",
            "Epoch 350/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.0973 - val_loss: 20.5072\n",
            "Epoch 351/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.7884 - val_loss: 19.2430\n",
            "Epoch 352/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.6675 - val_loss: 20.4203\n",
            "Epoch 353/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.8850 - val_loss: 20.7357\n",
            "Epoch 354/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 18.9266 - val_loss: 18.5263\n",
            "Epoch 355/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.7550 - val_loss: 21.9125\n",
            "Epoch 356/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.5425 - val_loss: 21.5052\n",
            "Epoch 357/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.4455 - val_loss: 17.8600\n",
            "Epoch 358/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.1606 - val_loss: 20.1511\n",
            "Epoch 359/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.9555 - val_loss: 20.4278\n",
            "Epoch 360/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.8267 - val_loss: 19.3223\n",
            "Epoch 361/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.1841 - val_loss: 19.1797\n",
            "Epoch 362/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.3867 - val_loss: 20.8844\n",
            "Epoch 363/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.7282 - val_loss: 19.2694\n",
            "Epoch 364/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.5757 - val_loss: 21.6454\n",
            "Epoch 365/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.0889 - val_loss: 20.2241\n",
            "Epoch 366/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.7131 - val_loss: 18.5944\n",
            "Epoch 367/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.3522 - val_loss: 20.9031\n",
            "Epoch 368/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.7067 - val_loss: 19.6363\n",
            "Epoch 369/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.9118 - val_loss: 19.4345\n",
            "Epoch 370/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.6219 - val_loss: 20.9358\n",
            "Epoch 371/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.4282 - val_loss: 20.3894\n",
            "Epoch 372/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.1056 - val_loss: 20.9484\n",
            "Epoch 373/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 18.7101 - val_loss: 19.4927\n",
            "Epoch 374/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 21.1893 - val_loss: 21.0341\n",
            "Epoch 375/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.7595 - val_loss: 19.3331\n",
            "Epoch 376/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.7496 - val_loss: 20.8509\n",
            "Epoch 377/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.2764 - val_loss: 19.6964\n",
            "Epoch 378/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.1928 - val_loss: 20.8849\n",
            "Epoch 379/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 18.5755 - val_loss: 20.6446\n",
            "Epoch 380/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.7905 - val_loss: 19.1288\n",
            "Epoch 381/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.3329 - val_loss: 21.0227\n",
            "Epoch 382/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.0973 - val_loss: 20.8830\n",
            "Epoch 383/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.7017 - val_loss: 21.3277\n",
            "Epoch 384/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.2493 - val_loss: 19.8035\n",
            "Epoch 385/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 18.8577 - val_loss: 18.5758\n",
            "Epoch 386/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.1075 - val_loss: 21.4415\n",
            "Epoch 387/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.2353 - val_loss: 19.0301\n",
            "Epoch 388/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.6449 - val_loss: 20.7283\n",
            "Epoch 389/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.0648 - val_loss: 19.7143\n",
            "Epoch 390/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.1056 - val_loss: 21.2909\n",
            "Epoch 391/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.6842 - val_loss: 18.0222\n",
            "Epoch 392/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.8566 - val_loss: 21.9826\n",
            "Epoch 393/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.5640 - val_loss: 20.8453\n",
            "Epoch 394/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.9557 - val_loss: 19.4815\n",
            "Epoch 395/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.7498 - val_loss: 18.9635\n",
            "Epoch 396/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.8691 - val_loss: 20.2991\n",
            "Epoch 397/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.1152 - val_loss: 21.1192\n",
            "Epoch 398/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.9184 - val_loss: 17.2193\n",
            "Epoch 399/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.4615 - val_loss: 22.2386\n",
            "Epoch 400/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.9367 - val_loss: 17.5731\n",
            "Epoch 401/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.6830 - val_loss: 20.7831\n",
            "Epoch 402/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.2064 - val_loss: 20.3010\n",
            "Epoch 403/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.3417 - val_loss: 18.3942\n",
            "Epoch 404/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.9532 - val_loss: 20.4940\n",
            "Epoch 405/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.0312 - val_loss: 20.6248\n",
            "Epoch 406/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.9972 - val_loss: 19.9062\n",
            "Epoch 407/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.3903 - val_loss: 19.5139\n",
            "Epoch 408/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.9292 - val_loss: 21.2097\n",
            "Epoch 409/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.2844 - val_loss: 20.6236\n",
            "Epoch 410/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.0256 - val_loss: 19.2543\n",
            "Epoch 411/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.1513 - val_loss: 20.8073\n",
            "Epoch 412/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.4239 - val_loss: 19.6148\n",
            "Epoch 413/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.7016 - val_loss: 21.9342\n",
            "Epoch 414/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 18.8726 - val_loss: 19.1413\n",
            "Epoch 415/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.2618 - val_loss: 20.4500\n",
            "Epoch 416/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.8432 - val_loss: 20.6995\n",
            "Epoch 417/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.7853 - val_loss: 20.2691\n",
            "Epoch 418/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 18.8366 - val_loss: 19.7625\n",
            "Epoch 419/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.9546 - val_loss: 19.5668\n",
            "Epoch 420/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.3937 - val_loss: 20.3529\n",
            "Epoch 421/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.8317 - val_loss: 20.4139\n",
            "Epoch 422/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.6962 - val_loss: 20.0934\n",
            "Epoch 423/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.6277 - val_loss: 20.6493\n",
            "Epoch 424/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.9883 - val_loss: 20.3672\n",
            "Epoch 425/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.8603 - val_loss: 19.0776\n",
            "Epoch 426/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.6685 - val_loss: 22.4160\n",
            "Epoch 427/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.5800 - val_loss: 18.1364\n",
            "Epoch 428/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.5205 - val_loss: 20.5428\n",
            "Epoch 429/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.1574 - val_loss: 19.6829\n",
            "Epoch 430/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.7302 - val_loss: 20.5350\n",
            "Epoch 431/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.6999 - val_loss: 20.9864\n",
            "Epoch 432/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.6952 - val_loss: 19.7839\n",
            "Epoch 433/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.0233 - val_loss: 20.5650\n",
            "Epoch 434/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.7788 - val_loss: 19.7378\n",
            "Epoch 435/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 18.8909 - val_loss: 18.3740\n",
            "Epoch 436/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.5538 - val_loss: 20.3417\n",
            "Epoch 437/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.4534 - val_loss: 20.2600\n",
            "Epoch 438/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 18.9026 - val_loss: 20.6345\n",
            "Epoch 439/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 18.8782 - val_loss: 18.9084\n",
            "Epoch 440/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.5983 - val_loss: 19.2875\n",
            "Epoch 441/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.9041 - val_loss: 19.6795\n",
            "Epoch 442/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.5134 - val_loss: 19.4102\n",
            "Epoch 443/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.9873 - val_loss: 19.6741\n",
            "Epoch 444/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 18.7104 - val_loss: 18.5419\n",
            "Epoch 445/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.1648 - val_loss: 22.0513\n",
            "Epoch 446/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.6448 - val_loss: 17.5334\n",
            "Epoch 447/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.3659 - val_loss: 20.1070\n",
            "Epoch 448/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.6571 - val_loss: 20.6351\n",
            "Epoch 449/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.8558 - val_loss: 20.4531\n",
            "Epoch 450/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.5039 - val_loss: 19.0795\n",
            "Epoch 451/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.8509 - val_loss: 18.2843\n",
            "Epoch 452/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.5707 - val_loss: 21.6718\n",
            "Epoch 453/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.5599 - val_loss: 19.4581\n",
            "Epoch 454/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.3129 - val_loss: 21.1691\n",
            "Epoch 455/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.2817 - val_loss: 20.0949\n",
            "Epoch 456/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.6938 - val_loss: 18.7373\n",
            "Epoch 457/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.1185 - val_loss: 21.5041\n",
            "Epoch 458/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 20.2187 - val_loss: 17.6758\n",
            "Epoch 459/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.5114 - val_loss: 19.3040\n",
            "Epoch 460/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.9940 - val_loss: 20.4492\n",
            "Epoch 461/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.4040 - val_loss: 20.2022\n",
            "Epoch 462/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.8646 - val_loss: 19.8903\n",
            "Epoch 463/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.4823 - val_loss: 18.2976\n",
            "Epoch 464/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 18.7937 - val_loss: 22.3501\n",
            "Epoch 465/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.9245 - val_loss: 18.6681\n",
            "Epoch 466/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.6846 - val_loss: 20.1045\n",
            "Epoch 467/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 18.9957 - val_loss: 19.6348\n",
            "Epoch 468/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.5913 - val_loss: 20.0208\n",
            "Epoch 469/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.0724 - val_loss: 20.0301\n",
            "Epoch 470/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.0209 - val_loss: 18.4738\n",
            "Epoch 471/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.1100 - val_loss: 21.8024\n",
            "Epoch 472/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 18.7092 - val_loss: 18.8909\n",
            "Epoch 473/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.3272 - val_loss: 19.6793\n",
            "Epoch 474/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.2636 - val_loss: 19.5469\n",
            "Epoch 475/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.1895 - val_loss: 18.5390\n",
            "Epoch 476/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 18.6594 - val_loss: 19.3614\n",
            "Epoch 477/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.7600 - val_loss: 20.7734\n",
            "Epoch 478/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.0366 - val_loss: 19.3414\n",
            "Epoch 479/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.6998 - val_loss: 20.5379\n",
            "Epoch 480/500\n",
            "22/22 [==============================] - 26s 1s/step - loss: 19.8513 - val_loss: 19.0139\n",
            "Epoch 481/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 18.7016 - val_loss: 20.2559\n",
            "Epoch 482/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.7564 - val_loss: 17.9153\n",
            "Epoch 483/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.4581 - val_loss: 21.0288\n",
            "Epoch 484/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.3811 - val_loss: 19.4332\n",
            "Epoch 485/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.5341 - val_loss: 19.7847\n",
            "Epoch 486/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.2904 - val_loss: 19.0525\n",
            "Epoch 487/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.4805 - val_loss: 19.9476\n",
            "Epoch 488/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.0742 - val_loss: 20.8452\n",
            "Epoch 489/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 18.5569 - val_loss: 17.5956\n",
            "Epoch 490/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.3250 - val_loss: 22.2081\n",
            "Epoch 491/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.2953 - val_loss: 18.2769\n",
            "Epoch 492/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.1985 - val_loss: 20.4365\n",
            "Epoch 493/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.4811 - val_loss: 18.8113\n",
            "Epoch 494/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.7055 - val_loss: 19.9596\n",
            "Epoch 495/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.2521 - val_loss: 20.2604\n",
            "Epoch 496/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 18.6298 - val_loss: 19.6495\n",
            "Epoch 497/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 20.0038 - val_loss: 18.4736\n",
            "Epoch 498/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.2693 - val_loss: 21.6679\n",
            "Epoch 499/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 19.3731 - val_loss: 19.4554\n",
            "Epoch 500/500\n",
            "22/22 [==============================] - 27s 1s/step - loss: 18.7467 - val_loss: 21.3253\n",
            "Unfreeze all of the layers.\n",
            "Train on 364 samples, val on 90 samples, with batch size 16.\n",
            "Epoch 51/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 20.5924 - val_loss: 19.3879\n",
            "Epoch 52/100\n",
            "22/22 [==============================] - 30s 1s/step - loss: 20.2090 - val_loss: 20.1883\n",
            "Epoch 53/100\n",
            "22/22 [==============================] - 32s 1s/step - loss: 18.8327 - val_loss: 17.2397\n",
            "Epoch 54/100\n",
            "22/22 [==============================] - 33s 1s/step - loss: 18.3245 - val_loss: 18.5973\n",
            "Epoch 55/100\n",
            "22/22 [==============================] - 33s 1s/step - loss: 18.2430 - val_loss: 17.3480\n",
            "Epoch 56/100\n",
            "22/22 [==============================] - 33s 1s/step - loss: 17.8250 - val_loss: 18.0365\n",
            "\n",
            "Epoch 00056: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "Epoch 57/100\n",
            "22/22 [==============================] - 31s 1s/step - loss: 17.0050 - val_loss: 17.3841\n",
            "Epoch 58/100\n",
            "22/22 [==============================] - 32s 1s/step - loss: 16.4205 - val_loss: 16.3273\n",
            "Epoch 59/100\n",
            "22/22 [==============================] - 33s 1s/step - loss: 16.5952 - val_loss: 16.2440\n",
            "Epoch 60/100\n",
            "22/22 [==============================] - 33s 1s/step - loss: 16.1565 - val_loss: 16.3169\n",
            "Epoch 61/100\n",
            "22/22 [==============================] - 32s 1s/step - loss: 15.8767 - val_loss: 15.8351\n",
            "Epoch 62/100\n",
            "22/22 [==============================] - 33s 1s/step - loss: 17.1651 - val_loss: 17.3139\n",
            "Epoch 63/100\n",
            "22/22 [==============================] - 32s 1s/step - loss: 14.9529 - val_loss: 17.2450\n",
            "Epoch 64/100\n",
            "22/22 [==============================] - 33s 1s/step - loss: 16.1129 - val_loss: 15.6276\n",
            "Epoch 65/100\n",
            "22/22 [==============================] - 33s 1s/step - loss: 15.4044 - val_loss: 16.8424\n",
            "Epoch 66/100\n",
            "22/22 [==============================] - 32s 1s/step - loss: 16.5342 - val_loss: 15.1616\n",
            "Epoch 67/100\n",
            "22/22 [==============================] - 32s 1s/step - loss: 15.6710 - val_loss: 16.9117\n",
            "Epoch 68/100\n",
            "22/22 [==============================] - 33s 1s/step - loss: 15.2791 - val_loss: 16.7904\n",
            "Epoch 69/100\n",
            "22/22 [==============================] - 32s 1s/step - loss: 15.9779 - val_loss: 15.9387\n",
            "\n",
            "Epoch 00069: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "Epoch 70/100\n",
            "22/22 [==============================] - 33s 1s/step - loss: 15.6832 - val_loss: 16.3188\n",
            "Epoch 71/100\n",
            "22/22 [==============================] - 32s 1s/step - loss: 15.9567 - val_loss: 16.3430\n",
            "Epoch 72/100\n",
            "22/22 [==============================] - 33s 1s/step - loss: 15.9992 - val_loss: 15.4623\n",
            "\n",
            "Epoch 00072: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
            "Epoch 73/100\n",
            "22/22 [==============================] - 33s 1s/step - loss: 15.4981 - val_loss: 16.3885\n",
            "Epoch 74/100\n",
            "22/22 [==============================] - 33s 1s/step - loss: 16.2024 - val_loss: 15.5812\n",
            "Epoch 75/100\n",
            "22/22 [==============================] - 33s 1s/step - loss: 15.6561 - val_loss: 16.6683\n",
            "\n",
            "Epoch 00075: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
            "Epoch 76/100\n",
            "22/22 [==============================] - 32s 1s/step - loss: 15.6843 - val_loss: 15.6247\n",
            "Epoch 00076: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypbIomiCLxV-",
        "colab_type": "text"
      },
      "source": [
        "# Save to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ajGCdRkL2Mk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3058162b-4080-48a1-fd96-87476dce85b9"
      },
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnGFUmK3MByX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a copy of the weights file with a datetime \n",
        "# and move that file to your own Drive\n",
        "%cp ./logs/000/trained_weights_stage_1.h5 ./logs/000/trained_weights_stage_1_$(date +%F-%H:%M).h5\n",
        "%mv ./logs/000/trained_weights_stage_1_$(date +%F-%H:%M).h5 /content/drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}